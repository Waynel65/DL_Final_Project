{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Make sure you restart runtime after installing the libraries below.\n"
      ],
      "metadata": {
        "id": "5cTEmXUi5XcN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1fRPOTVtqhH",
        "outputId": "861ddc86-61cd-49c4-9fcc-31f6c784b374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa==0.9.2 in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2) (1.2.0)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2) (0.4.2)\n",
            "Requirement already satisfied: numba>=0.45.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2) (23.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.45.1->librosa==0.9.2) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.45.1->librosa==0.9.2) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa==0.9.2) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa==0.9.2) (2.27.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->librosa==0.9.2) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.10.2->librosa==0.9.2) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.2) (2.21)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.10/dist-packages (0.20.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.1)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (2.13.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (0.70.14)\n",
            "Requirement already satisfied: dill>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from multiprocess) (0.3.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaldiio in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from kaldiio) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: webrtcvad in /usr/local/lib/python3.10/dist-packages (2.0.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: utils in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ],
      "source": [
        "## installing the required packages\n",
        "!pip install librosa==0.9.2\n",
        "!pip install transformers\n",
        "!pip install unidecode\n",
        "!pip install tensorflow_addons\n",
        "!pip install multiprocess\n",
        "!pip install kaldiio\n",
        "!pip install webrtcvad\n",
        "!pip install utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting google drive for the session\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA1QYnRN933M",
        "outputId": "1ad276b5-3600-415f-e987-b90d8542ee6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/DL_Final_Project/original_repo/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g015wW8oNat6",
        "outputId": "67fdfa94-d8e6-4d27-b98e-9da1de6a7a87"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DL_Final_Project/original_repo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will only need to change the `repo_path` below so that it points to the folder that contains source code from the repo\n"
      ],
      "metadata": {
        "id": "NfrMJYHo5qPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTS\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModel\n",
        "import librosa\n",
        "\n",
        "from synthesizer.inference import Synthesizer\n",
        "from synthesizer_like_translator.inference import Synthesizer as Translator\n",
        "from synthesizer.kaldi_interface import KaldiInterface\n",
        "from encoder import inference as encoder_speaker\n",
        "from encoder import inference_accent as encoder_accent\n",
        "from vocoder import inference as vocoder\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from utils.argutils import print_args\n",
        "import random\n",
        "import IPython.display as ipd\n",
        "import os\n",
        "from synthesizer.hparams import hparams\n",
        "\n",
        "## paths set up ##\n",
        "\n",
        "repo_path = '/content/drive/MyDrive/DL_Final_Project/original_repo/' ## CHANGE THIS\n",
        "\n",
        "### SHOULDNT NEED TO CHANGE THESE ###\n",
        "pretrained_models_path = os.path.join(repo_path, \"pretrained_models/\")\n",
        "encoder_speaker_weights = Path(os.path.join(pretrained_models_path, \"pretrained_model/pretrained/encoder/saved_models/pretrained.pt\"))\n",
        "encoder_accent_weights = Path(os.path.join(pretrained_models_path, \"pretrained_model/pretrained/encoder/saved_models/encoder_accent.pt\"))\n",
        "vocoder_weights = Path(os.path.join(pretrained_models_path, \"pretrained_model/pretrained/vocoder/saved_models/pretrained/pretrained.pt\"))\n",
        "syn_dir = Path(os.path.join(pretrained_models_path, \"synthesizer/taco_pretrained\"))\n",
        "syn_dir_trans = Path(os.path.join(pretrained_models_path, \"translator/logs-translator_train/taco_pretrained\"))\n",
        "### SHOULDNT NEED TO CHANGE THESE ###\n",
        "\n",
        "## loading pretrained models for accent encoder and speaker coder\n",
        "encoder_speaker.load_model(encoder_speaker_weights)\n",
        "encoder_accent.load_model(encoder_accent_weights)\n",
        "# synthesizer = Synthesizer(syn_dir)\n",
        "# translator = Translator(syn_dir_trans)\n",
        "# vocoder.load_model(vocoder_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNzJUemgJAsT",
        "outputId": "d5e417eb-e432-49df-80d5-4bcab2d759f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded encoder \"pretrained.pt\" trained to step 1564501\n",
            "Loaded encoder \"encoder_accent.pt\" trained to step 90001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def synthesize(bnf, embed):\n",
        "#     spec = synthesizer.synthesize_spectrograms([bnf], [embed])[0]\n",
        "#     # print(spec.shape)\n",
        "#     generated_wav = vocoder.infer_waveform(spec)\n",
        "#     generated_wav = np.pad(generated_wav, (0, synthesizer.sample_rate), mode=\"constant\")\n",
        "#     return generated_wav\n",
        "\n",
        "\n",
        "# def translate_ppg(bnf, embed):\n",
        "#     spec = translator.synthesize_spectrograms([bnf], [embed])[0]\n",
        "#     return spec\n",
        "\n",
        "# We will be using these functions that use the pretrained accent/speaker encoder to generate the accent/speaker embeddings\n",
        "def generate_accent_embed(src_utterance_path):\n",
        "    wav, _ = librosa.load(src_utterance_path, hparams.sample_rate)\n",
        "    wav = encoder_speaker.preprocess_wav(wav)\n",
        "    embed_accent = encoder_accent.embed_utterance(wav)\n",
        "\n",
        "    return embed_accent\n",
        "\n",
        "def generate_speaker_embed(tgt_utterance_path):\n",
        "    wav, _ = librosa.load(tgt_utterance_path, hparams.sample_rate)\n",
        "    wav = encoder_speaker.preprocess_wav(wav)\n",
        "    embed_speaker = encoder_speaker.embed_utterance(wav)\n",
        "\n",
        "    return embed_speaker"
      ],
      "metadata": {
        "id": "DYOdgNSDRqfh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:"
      ],
      "metadata": {
        "id": "B76VRPGD6DTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Since we extracted the BNFs on a local machine, the scp files record the absolute paths to where the BNFs are. And obviously this will not work on google colab because now we have a completely different set of paths (I have tried to use relative paths on local machine but because of how the kaldi script work, it would not work)\n",
        "\n",
        "- We will need to preprocess the scp files again so that they have google drive paths\n",
        "\n",
        "- Run the functions below before loading their BNFs"
      ],
      "metadata": {
        "id": "LyqYiP-7edK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_wav_scp(speaker_path):\n",
        "  \"\"\"\n",
        "    input example: /content/drive/MyDrive/DL_Final_Project/speakers/L2_speakers/BWC/\n",
        "  \"\"\"\n",
        "  kaldi_dir = speaker_path+\"kaldi/\"\n",
        "  scp_path = str(os.path.join(kaldi_dir, 'wav.scp'))\n",
        "  new_scp_path =  str(os.path.join(kaldi_dir, 'wav_modified.scp'))\n",
        "  print(scp_path)\n",
        "\n",
        "  with open(scp_path, \"r\") as old_scp, open(new_scp_path, \"w\") as new_scp:\n",
        "        for line in old_scp:\n",
        "            print(line)\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != 2:\n",
        "                print(f\"Warning: Invalid line in {scp_path}: {line}\")\n",
        "                continue\n",
        "            file_id, old_filepath = parts\n",
        "            # old_filepath is like \"/home/jb7410/ppg2ppg/speakers/L2_speakers/BWC/wav/arctic_a0011.wav\"\n",
        "            head, tail = os.path.split(old_filepath)  \n",
        "            _, tail2 = os.path.split(head)\n",
        "            keep_part = os.path.join(tail2, tail) \n",
        "            \n",
        "            new_filepath = os.path.join(speaker_path, keep_part)\n",
        "            new_scp.write(f\"{file_id} {new_filepath}\\n\")\n",
        "        print(f\"Modified scp file saved at: {new_scp_path}\")\n",
        "\n",
        "def preprocess_feats_scp(speaker_path):\n",
        "  \"\"\"\n",
        "    input example: /content/drive/MyDrive/DL_Final_Project/speakers/L2_speakers/BWC/\n",
        "  \"\"\"\n",
        "  bnf_dir = speaker_path+\"kaldi/bnf/\"\n",
        "  scp_path = str(os.path.join(bnf_dir, 'feats.scp'))\n",
        "  new_scp_path =  str(os.path.join(bnf_dir, 'feats_modified.scp'))\n",
        "  print(scp_path)\n",
        "\n",
        "  with open(scp_path, \"r\") as old_scp, open(new_scp_path, \"w\") as new_scp:\n",
        "      for line in old_scp:\n",
        "          print(line)\n",
        "          parts = line.strip().split()\n",
        "          if len(parts) != 2:\n",
        "              print(f\"Warning: Invalid line in {scp_path}: {line}\")\n",
        "              continue\n",
        "          file_id, old_filepath = parts\n",
        "          # old_filepath is like \"/home/jb7410/ppg2ppg/speakers/L2_speakers/BWC/kaldi/bnf/data/raw_bnfeat_kaldi.1.ark:4248913\"\n",
        "          _, tail = os.path.split(old_filepath)  \n",
        "\n",
        "          new_filepath = os.path.join(bnf_dir+\"data/\", tail)\n",
        "          print(new_filepath)\n",
        "          new_scp.write(f\"{file_id} {new_filepath}\\n\")\n",
        "      print(f\"Modified scp file saved at: {new_scp_path}\")\n",
        "\n",
        "\n",
        "def preprocess_both(speaker_path):\n",
        "  preprocess_wav_scp(speaker_path)\n",
        "  preprocess_feats_scp(speaker_path)\n",
        "\n",
        "  return\n",
        "\n"
      ],
      "metadata": {
        "id": "K6ofy6rFfHEo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## We are combining scp files into one big scp file so that it will be easier for us to load data when using the pytorch dataloader\n",
        "\n",
        "def combine_feats(L2_speaker_path):\n",
        "  combined_feats_path = os.path.join(L2_speaker_path, 'combined_feats.scp')\n",
        "  \n",
        "  # Ensure combined_feats.scp is empty before writing\n",
        "  open(combined_feats_path, 'w').close()\n",
        "\n",
        "  for speaker_id in os.listdir(L2_speaker_path):\n",
        "      print(f\"taking utterances from {speaker_id} now\")\n",
        "      speaker_feats_path = os.path.join(L2_speaker_path, speaker_id, 'kaldi', 'bnf', 'feats_modified.scp')\n",
        "      \n",
        "      if os.path.isfile(speaker_feats_path):\n",
        "          with open(speaker_feats_path, 'r') as speaker_feats_file:\n",
        "              lines = speaker_feats_file.readlines()\n",
        "              \n",
        "          with open(combined_feats_path, 'a') as combined_feats_file:\n",
        "              combined_feats_file.writelines(lines)\n",
        "\n",
        "def combine_wavs(L2_speaker_path):\n",
        "    combined_feats_path = os.path.join(L2_speaker_path, 'combined_wav.scp')\n",
        "  \n",
        "    # Ensure combined_feats.scp is empty before writing\n",
        "    open(combined_feats_path, 'w').close()\n",
        "\n",
        "    for speaker_id in os.listdir(L2_speaker_path):\n",
        "        print(f\"taking utterances from {speaker_id} now\")\n",
        "        speaker_feats_path = os.path.join(L2_speaker_path, speaker_id, 'kaldi','wav_modified.scp')\n",
        "        \n",
        "        if os.path.isfile(speaker_feats_path):\n",
        "            with open(speaker_feats_path, 'r') as speaker_feats_file:\n",
        "                lines = speaker_feats_file.readlines()\n",
        "                \n",
        "            with open(combined_feats_path, 'a') as combined_feats_file:\n",
        "                combined_feats_file.writelines(lines)"
      ],
      "metadata": {
        "id": "3hkcp2Ks-3KM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_acc_embeds(combined_wav_scp_path, output_dir, embed_scp_path):\n",
        "  \"\"\"\n",
        "    this function reads in all the L2 speakers' wav files (definied in the combined_wav.scp)\n",
        "    generate accent embeddings for all of them\n",
        "    and store the embeddings in the disk\n",
        "  \"\"\"\n",
        "  if not os.path.exists(output_dir):\n",
        "      os.makedirs(output_dir)\n",
        "\n",
        "  with open(combined_wav_scp_path, 'r') as combined_wav_scp_file:\n",
        "      lines = combined_wav_scp_file.readlines()\n",
        "\n",
        "  with open(embed_scp_path, 'w') as embed_scp_file:\n",
        "      for line in lines:\n",
        "          scp_id, wav_file_path = line.strip().split()  # assuming the format is: \"id path\"\n",
        "\n",
        "          # Check if the wav file exists\n",
        "          if os.path.isfile(wav_file_path):\n",
        "              accent_embed = generate_accent_embed(wav_file_path)\n",
        "              output_path = os.path.join(output_dir, scp_id + '.npy')\n",
        "              np.save(output_path, accent_embed)\n",
        "\n",
        "              # Write the scp_id and corresponding embedding file path to the embeddings scp file\n",
        "              embed_scp_file.write(f'{scp_id} {output_path}\\n')\n",
        "\n",
        "  print(\"Saved accent embeddings in '{}'.\".format(output_dir))"
      ],
      "metadata": {
        "id": "0KxPcQQt_ty8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## PRE-PROCESSING DATA:\n",
        "\n",
        "# preprocess_feats_scp(\"/content/drive/MyDrive/DL_Final_Project/speakers/L2_speakers/BWC/\")\n",
        "# preprocess_wav_scp(\"/content/drive/MyDrive/DL_Final_Project/speakers/L1_speakers/BDL/\")\n",
        "# preprocess_feats_scp(\"/content/drive/MyDrive/DL_Final_Project/speakers/L1_speakers/BDL/\")\n",
        "\n",
        "# preprocess_both(\"/content/drive/MyDrive/DL_Final_Project/speakers/L2_speakers/ABA/\")\n",
        "# preprocess_both(\"/content/drive/MyDrive/DL_Final_Project/speakers/L2_speakers/EBVS/\")\n",
        "# preprocess_both(\"/content/drive/MyDrive/DL_Final_Project/speakers/L2_speakers/HQTV/\")\n",
        "\n",
        "# combine_feats(\"/content/drive/MyDrive/DL_Final_Project/speakers/L2_speakers/\")\n",
        "# combine_wavs(\"/content/drive/MyDrive/DL_Final_Project/speakers/L2_speakers/\")\n",
        "# gen_acc_embeds(\"/content/drive/MyDrive/DL_Final_Project/speakers/L2_speakers/combined_wav.scp\", \"/content/drive/MyDrive/DL_Final_Project/speakers/L2_speakers/acc_embeds\", \"/content/drive/MyDrive/DL_Final_Project/speakers/L2_speakers/accent_embeds.scp\")\n"
      ],
      "metadata": {
        "id": "FC4eLsyDr7ED"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## FUNCTION TO LOAD BNFS AND SEE IF THEY LOAD PROPERLY\n",
        "\n",
        "def load_bnfs_single_utt(speaker_path, speaker_id, utterance_id):\n",
        "  \"\"\"\n",
        "    speaker path example: /content/drive/MyDrive/DL_Final_Project/speakers/L1_speakers/\n",
        "    speaker_id example: \"BDL\"\n",
        "  \"\"\"\n",
        "\n",
        "  kaldi_dir = f\"{speaker_path}{speaker_id}/kaldi\"\n",
        "  ki = KaldiInterface(wav_scp=str(os.path.join(kaldi_dir, 'wav_modified.scp')),\n",
        "                    bnf_scp=str(os.path.join(kaldi_dir, 'bnf/feats_modified.scp')))\n",
        "  bnf = ki.get_feature('_'.join([speaker_id, utterance_id]), 'bnf')\n",
        "\n",
        "  return bnf\n",
        "\n",
        "def load_bnfs_all_utt(speaker_path, speaker_id):\n",
        "  \"\"\"\n",
        "    get all utterances' bnfs for a speaker\n",
        "  \"\"\"\n",
        "\n",
        "  kaldi_dir = f\"{speaker_path}{speaker_id}/kaldi\"\n",
        "\n",
        "  ## need to count the number of utterances in scp file\n",
        "  feat_file = f\"{kaldi_dir}/bnf/feats_modified.scp\"\n",
        "  assert os.path.exists(feat_file)\n",
        "\n",
        "  result = []\n",
        "  with open(feat_file, 'r') as file:\n",
        "     for line in file:\n",
        "      utterance_id, _ = line.strip().split()\n",
        "      utterance_id = utterance_id.replace(\"BDL_\", \"\")\n",
        "      print(utterance_id)\n",
        "      bnf = load_bnfs_single_utt(speaker_path, speaker_id, utterance_id)\n",
        "      result.append(bnf)\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "# bnf = load_bnfs_single_utt(\"/content/drive/MyDrive/DL_Final_Project/speakers/L1_speakers/\", \"BDL\", 'arctic_b0503')\n",
        "# results = load_bnfs_all_utt(\"/content/drive/MyDrive/DL_Final_Project/speakers/L1_speakers/\", \"BDL\")\n",
        "# results[50].shape\n",
        "\n"
      ],
      "metadata": {
        "id": "3ZBD4_7hNvrJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from scipy.io import wavfile\n",
        "import numpy as np\n",
        "\n",
        "### PLAYING AROUND WITH BNFS ###\n",
        "\n",
        "### CHANGE THESE AS NEEDED ###\n",
        "utterance_id = 'arctic_b0503'\n",
        "L1_speaker_id = \"BDL\"\n",
        "L1_speaker_path = \"/content/drive/MyDrive/DL_Final_Project/speakers/L1_speakers/\"\n",
        "L2_speaker_id = 'BWC'\n",
        "L2_speaker_path = \"/content/drive/MyDrive/DL_Final_Project/speakers/L2_speakers/\"\n",
        "\n",
        "L1_wav_path =f\"{L1_speaker_path}{L1_speaker_id}/wav/\"\n",
        "\n",
        "L2_wav_path = f\"{L2_speaker_path}{L2_speaker_id}/wav/\"\n",
        "### CHANGE THESE AS NEEDED ###\n",
        "\n",
        "## Loading the BNFs\n",
        "L1_bnf = load_bnfs_single_utt(L1_speaker_path, L1_speaker_id, utterance_id)\n",
        "L2_bnf = load_bnfs_single_utt(L2_speaker_path, L2_speaker_id, utterance_id)\n",
        "\n",
        "## GETTING THE EMBEDDINGS\n",
        "acc_utterance_path = L1_wav_path + str(utterance_id) + '.wav'\n",
        "speak_utterance_path = L2_wav_path + str(utterance_id) + '.wav'\n",
        "embed_speaker = generate_speaker_embed(acc_utterance_path) # used accent utterance path just for the example\n",
        "embed_accent = generate_accent_embed(acc_utterance_path)\n",
        "\n",
        "print(L1_bnf.shape)\n",
        "print(L2_bnf.shape)\n",
        "print(embed_speaker.shape)"
      ],
      "metadata": {
        "id": "jxLEkyl_TG_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f2af415-7a47-4e94-cc1e-eecfbab512f3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-5140467d96d3>:22: FutureWarning: Pass sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  wav, _ = librosa.load(tgt_utterance_path, hparams.sample_rate)\n",
            "/content/drive/MyDrive/DL_Final_Project/original_repo/encoder/audio.py:47: FutureWarning: Pass y=[0.0012207  0.00146484 0.00128174 ... 0.         0.         0.        ], sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  frames = librosa.feature.melspectrogram(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(329, 256)\n",
            "(542, 256)\n",
            "(256,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-5140467d96d3>:15: FutureWarning: Pass sr=16000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  wav, _ = librosa.load(src_utterance_path, hparams.sample_rate)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architecture of our transformer based translator:\n",
        "- input: L2_BNFs + L2 Accent Embeddings\n",
        "- it is probably a good idea to pass the L2 accent embeddings after the BNFs have gone through the encoder layer\n",
        "- need to generate positional encoding (will also be part of input)\n",
        "- output: L1 BNFs\n",
        "\n",
        "We will be using a pretrained transformer model and finetune it to our needs\n",
        "\n"
      ],
      "metadata": {
        "id": "jkGA6gFx2C81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining our custom dataset class\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import warnings\n",
        "\n",
        "class BNFDataset(Dataset):\n",
        "  \"\"\"\n",
        "    here the L2 scp files should be a big concatenated list of all bnfs\n",
        "  \"\"\"\n",
        "  def __init__(self, L1_wav_scp, L2_wav_scp, L1_feats_scp, L2_feats_scp, accent_embds_scp):\n",
        "      # each of these should just be storing a path\n",
        "      # remember to provide the modified version of scp\n",
        "\n",
        "      self.L1_wav_scp = L1_wav_scp\n",
        "      self.L2_wav_scp = L2_wav_scp\n",
        "      self.L1_feats_scp = L1_feats_scp\n",
        "      self.L2_feats_scp = L2_feats_scp\n",
        "      self.embed_scp = accent_embds_scp\n",
        "      # self.L2_accent_embedding = L2_accent_embedding\n",
        "\n",
        "  def load_bnfs_single_utt(self, wav_scp, feats_scp, speaker_id, utterance_id):\n",
        "      ki = KaldiInterface(wav_scp=wav_scp, bnf_scp=feats_scp)\n",
        "      # print('_'.join([speaker_id, utterance_id]))\n",
        "      bnf = ki.get_feature('_'.join([speaker_id, utterance_id]), 'bnf')\n",
        "      \n",
        "      return bnf\n",
        "  \n",
        "\n",
        "  def load_scp_as_dict(self, scp_path):\n",
        "      scp_dict = {}\n",
        "      with open(scp_path, 'r') as f:\n",
        "          for line in f:\n",
        "              utterance_id, file_location = line.strip().split()\n",
        "              # split_key = key.split('_')\n",
        "              # utterance_id = str(split_key[1]) + '_' + str(split_key[2])\n",
        "              # print(utterance_id)\n",
        "              scp_dict[utterance_id] = file_location\n",
        "      return scp_dict\n",
        "  \n",
        "  def get_embeddings(self, artic_id):\n",
        "      embed_dict = self.load_scp_as_dict(self.embed_scp)\n",
        "      embed_file = embed_dict[artic_id]\n",
        "      return np.load(embed_file)\n",
        "      # wav_scp_dict = self.load_scp_as_dict(self.L2_wav_scp)\n",
        "      # wav_file = wav_scp_dict[artic_id]\n",
        "      # with warnings.catch_warnings():\n",
        "      #   warnings.simplefilter(\"ignore\")  # Ignore all warnings\n",
        "      #   embed_accent = generate_accent_embed(wav_file)\n",
        "\n",
        "      # print(f\"embed accent shape: {embed_accent.shape}\")\n",
        "      # return embed_accent\n",
        "\n",
        "  def __len__(self):\n",
        "      \"\"\"\n",
        "        this give the number of samples (i.e. the number of total L2 BNFs)\n",
        "      \"\"\"\n",
        "      l2_scp_dict = self.load_scp_as_dict(self.L2_feats_scp)\n",
        "      return len(l2_scp_dict)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      # L2_speaker_list = [\"ABA\", \"EBVS\", ]\n",
        "      L2_scp_dict = self.load_scp_as_dict(self.L2_feats_scp)\n",
        "      artic_id = list(L2_scp_dict.keys())[idx]\n",
        "      # print(f\"a random utterance: {artic_id}\")\n",
        "\n",
        "      split_utter = artic_id.split('_')\n",
        "      L2_speaker_id = split_utter[0]\n",
        "      utterance_id = str(split_utter[1]) + '_' + str(split_utter[2])\n",
        "      # rand_idx = random.randint(0, len(L2_speaker_list)-1)\n",
        "      # L2_speaker_id = L2_speaker_list[rand_idx]\n",
        "\n",
        "\n",
        "      # print(type(self.L1_wav_scp))\n",
        "\n",
        "      L1_bnf = self.load_bnfs_single_utt(self.L1_wav_scp, self.L1_feats_scp, \"BDL\", utterance_id)\n",
        "      L2_bnf = self.load_bnfs_single_utt(self.L2_wav_scp, self.L2_feats_scp, L2_speaker_id, utterance_id)\n",
        "      # print(artic_id)\n",
        "      L2_accent_embedding = self.get_embeddings(artic_id)\n",
        "      return L1_bnf, L2_bnf, L2_accent_embedding\n",
        "      # return L1_bnf, L2_bnf\n",
        "\n",
        "\n",
        "L1_wav_scp = \"/content/drive/MyDrive/DL_Final_Project/speakers/L1_speakers/BDL/kaldi/wav_modified.scp\"\n",
        "L2_wav_scp = \"/content/drive/MyDrive/DL_Final_Project/speakers/L2_speakers/combined_wav.scp\"\n",
        "L1_feats_scp = \"/content/drive/MyDrive/DL_Final_Project/speakers/L1_speakers/BDL/kaldi/bnf/feats_modified.scp\"\n",
        "L2_feats_scp = \"/content/drive/MyDrive/DL_Final_Project/speakers/L2_speakers/combined_feats.scp\"\n",
        "accent_embds_scp = \"/content/drive/MyDrive/DL_Final_Project/speakers/L2_speakers/accent_embeds.scp\"\n",
        "\n",
        "dataset = BNFDataset(L1_wav_scp, L2_wav_scp, L1_feats_scp, L2_feats_scp, accent_embds_scp)\n",
        "dataset.__getitem__(500)"
      ],
      "metadata": {
        "id": "JgwdOVRxc5bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "from torch.utils.data import random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class CollateFunction:\n",
        "    \"\"\"\n",
        "      we define how we pad the our input bnfs so that they can be stacked in the dataloader\n",
        "      (i.e. the bnfs in each batch will have bnfs of same length)\n",
        "    \"\"\"\n",
        "    def __init__(self, padding_value=0):\n",
        "        self.padding_value = padding_value\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # Separate the input data and targets\n",
        "        targets, inputs , acc_embed = zip(*batch)\n",
        "\n",
        "        # Pad the input data\n",
        "        inputs = pad_sequence([torch.tensor(data) for data in inputs],\n",
        "                              batch_first=True, padding_value=self.padding_value)\n",
        "        \n",
        "        # Create attention masks\n",
        "        input_masks = (inputs != self.padding_value).long()\n",
        "        \n",
        "        # Pad the target data\n",
        "        targets = pad_sequence([torch.tensor(data) for data in targets],\n",
        "                               batch_first=True, padding_value=self.padding_value)\n",
        "        \n",
        "        acc_embed = torch.stack([torch.tensor(embed) for embed in acc_embed])\n",
        "\n",
        "\n",
        "        return inputs, acc_embed, input_masks, targets\n",
        "\n",
        "## get our custom dataset\n",
        "dataset = BNFDataset(L1_wav_scp, L2_wav_scp, L1_feats_scp, L2_feats_scp, accent_embds_scp)\n",
        "\n",
        "# Let's say dataset is your Dataset object\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.8 * dataset_size)  # 80% for training\n",
        "test_size = dataset_size - train_size  # 20% for testing\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=CollateFunction(padding_value=0))\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=CollateFunction(padding_value=0))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HoLYeLbt67d0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ATTEMPT 1: Build a translator with Bert\n",
        "\n",
        "This part did not work due to the constraints that Bert only allows 512 tokens for each input while our inputs are much larger than that"
      ],
      "metadata": {
        "id": "S6-MkBwMBS3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## building translator transformer model\n",
        "# https://huggingface.co/docs/transformers/model_doc/encoder-decoder\n",
        "\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "class transformer_translator(nn.Module):\n",
        "  def __init__(self, accent_embedding_dim, output_dim=256): # output_dim: size of vectorized BNFs\n",
        "        super().__init__()\n",
        "\n",
        "        # Pretrained BERT model for the encoder\n",
        "        self.encoder = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "        # self.input_transform = nn.Conv1d(feature_size, self.encoder.config.hidden_size, 1)\n",
        "\n",
        "        # An additional layer to transform the accent embedding to match the BERT dimensionality\n",
        "        self.accent_transform = nn.Linear(accent_embedding_dim, self.encoder.config.hidden_size)\n",
        "\n",
        "        # Pretrained BERT model for the decoder\n",
        "        self.decoder = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "        # Final output layer\n",
        "        self.output_layer = nn.Linear(self.decoder.config.hidden_size, output_dim)\n",
        "\n",
        "\n",
        "  def forward(self, bnfs, accent_embeddings, input_masks):\n",
        "      # Pass the bnfs through the encoder\n",
        "      # bnfs = self.input_transform(bnfs.transpose(1, 2)).transpose(1, 2)\n",
        "      bnfs = bnfs.view(bnfs.size(0), -1)\n",
        "      encoder_output = self.encoder(bnfs, attention_mask=input_masks).last_hidden_state\n",
        "\n",
        "      # Transform the accent embedding and unsqueeze to match encoder output dimensions\n",
        "      transformed_accent = self.accent_transform(accent_embeddings).unsqueeze(1).repeat(1, encoder_output.size(1), 1)\n",
        "\n",
        "      # Concatenate the encoder output with the transformed accent embedding along the last dimension\n",
        "      concat = torch.cat((encoder_output, transformed_accent), dim=-1)\n",
        "\n",
        "      # Pass the concatenated tensor through the decoder\n",
        "      decoder_output = self.decoder(concat).last_hidden_state\n",
        "\n",
        "      # Pass the decoder output through the output layer to generate translated BNFs\n",
        "      translated_bnfs = self.output_layer(decoder_output)\n",
        "\n",
        "      return translated_bnfs\n"
      ],
      "metadata": {
        "id": "tq9fNoBB1kdx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#! testing with the bert model but it didn't work out:\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# batch = next(iter(train_loader))\n",
        "# bnfs, acc_embed, input_masks, targets = batch\n",
        "\n",
        "# bnfs = bnfs.to(device)\n",
        "# acc_embed = acc_embed.to(device)\n",
        "# input_masks = input_masks.to(device)\n",
        "# targets = targets.to(device)\n",
        "\n",
        "# print(bnfs.shape)\n",
        "# print(acc_embed.shape)\n",
        "# print(input_masks.shape)\n",
        "# model = transformer_translator(256).to(device)\n",
        "# outputs = model(bnfs, acc_embed, input_masks) => NOT WORKING\n"
      ],
      "metadata": {
        "id": "M_RZXIJ6QH3v"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# second attempt on translator model (with help of CHATGPT)\n",
        "\n",
        "# This class is used to implement Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # Apply dropout with a given probability\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create a zero matrix of shape [max_len, d_model]\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        # Create a position tensor of shape [max_len, 1]\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        # Apply sine to even indices in the array; 2i\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # Apply cosine to odd indices in the array; 2i+1\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        # Register pe as a buffer that should not to be considered a model parameter\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to the input\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        # Apply dropout\n",
        "        return self.dropout(x)\n",
        "\n",
        "class MyTransformer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward):\n",
        "        super(MyTransformer, self).__init__()\n",
        "        \n",
        "        # Define positional encoding for the encoder\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        # Define transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
        "        \n",
        "        # Define positional encoding for the decoder\n",
        "        self.pos_decoder = PositionalEncoding(d_model)\n",
        "        # Define transformer decoder\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers)\n",
        "        \n",
        "        # Define a linear transformation to map from (d_model + 256) to d_model\n",
        "        self.fc = nn.Linear(d_model+256, d_model)\n",
        "\n",
        "    def forward(self, src, tgt, accent_embeddings):\n",
        "        # Transpose the input to match the expected input shape for the encoder\n",
        "        src = src.transpose(0, 1)\n",
        "        # Apply positional encoding to the source input\n",
        "        src = self.pos_encoder(src)\n",
        "        \n",
        "        # Transpose the target to match the expected input shape for the decoder\n",
        "        tgt = tgt.transpose(0, 1)\n",
        "        # Apply positional encoding to the target input\n",
        "        tgt = self.pos_decoder(tgt)\n",
        "        \n",
        "        # Pass the source input through the transformer encoder\n",
        "        memory = self.transformer_encoder(src)\n",
        "\n",
        "        # Repeat and concatenate the accent embeddings to the encoder output\n",
        "        accent_embeddings = accent_embeddings.unsqueeze(0).repeat(src.size(0), 1, 1)\n",
        "        memory = torch.cat((memory, accent_embeddings), dim=-1)\n",
        "\n",
        "        # Pass the concatenated tensor through the linear layer\n",
        "        memory = self.fc(memory)\n",
        "\n",
        "        # Pass the target input and the transformed source input to the transformer decoder\n",
        "        out = self.transformer_decoder(tgt, memory)\n",
        "\n",
        "        # Transpose the output back to (N, T, E)\n",
        "        return out.transpose(0,1)\n",
        "\n"
      ],
      "metadata": {
        "id": "0VuL1SCN2mX8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## defining hyperparameters and model here\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch = next(iter(train_loader))\n",
        "bnfs, acc_embed, input_masks, targets = batch\n",
        "\n",
        "bnfs = bnfs.to(device)\n",
        "acc_embed = acc_embed.to(device)\n",
        "input_masks = input_masks.to(device)\n",
        "targets = targets.to(device)\n",
        "\n",
        "print(bnfs.shape)\n",
        "print(acc_embed.shape)\n",
        "print(input_masks.shape)\n",
        "\n",
        "# Hyperparameters\n",
        "d_model = 256  # Number of features in the input\n",
        "nhead = 8  # Number of attention heads\n",
        "num_encoder_layers = 4  # Number of layers in the encoder\n",
        "num_decoder_layers = 4  # Number of layers in the decoder\n",
        "dim_feedforward = 1024  # Dimensionality of the feedforward network\n",
        "\n",
        "# Create an instance of the model\n",
        "model = MyTransformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward)\n",
        "model.to(device)\n",
        "out = model(bnfs, targets, acc_embed)\n",
        "\n",
        "\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6shLRYJ9dRZ",
        "outputId": "5554cb50-9410-4b50-92d6-a7d5ec295309"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 811, 256])\n",
            "torch.Size([8, 256])\n",
            "torch.Size([8, 811, 256])\n",
            "torch.Size([8, 439, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "  \n",
        "print(\"Number of trainable parameters\",count_parameters(model))"
      ],
      "metadata": {
        "id": "RyC--r6L_O_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4c050ed-9ef5-4d84-ce22-35c5e3c5e91d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters 7504128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "        bnfs, acc_embed, input_masks, targets = batch\n",
        "        bnfs = bnfs.to(device)\n",
        "        acc_embed = acc_embed.to(device)\n",
        "        input_masks = input_masks.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Reset the gradients\n",
        "        output = model(bnfs, targets, acc_embed)  # Forward pass\n",
        "        loss = criterion(output, targets)  # Compute the loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update the weights\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_loader)  # Return the average loss\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to track gradients in evaluation mode\n",
        "        for i, batch in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
        "            # print(f\"loading batch {i} for evaluation\")\n",
        "            bnfs, acc_embed, input_masks, targets = batch\n",
        "            bnfs = bnfs.to(device)\n",
        "            acc_embed = acc_embed.to(device)\n",
        "            input_masks = input_masks.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            output = model(bnfs, targets, acc_embed)  # Forward pass\n",
        "            loss = criterion(output, targets)  # Compute the loss\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            # print(f\"finished batch {i} for evaluation\")\n",
        "    return total_loss / len(test_loader)  # Return the average loss"
      ],
      "metadata": {
        "id": "ydFtszvE-PJv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare your data here (BNFs, Mel-spectrograms, and speaker embeddings)\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs):\n",
        "    best_val_loss = 99999\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "        # test_loss = evaluate(model, val_loader, criterion, device)\n",
        "        train_losses.append(train_loss)\n",
        "        # test_losses.append(test_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "        # print(f'\\t Test. Loss: {test_loss:.3f}')\n",
        "        if train_loss < best_val_loss:\n",
        "          best_val_loss = train_loss\n",
        "          torch.save(model.state_dict(), \"best_model.pth\")\n",
        "          print(\"Best model saved!\")"
      ],
      "metadata": {
        "id": "BDP2_O1i-d8P"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Specify the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 20\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, test_loader, criterion, optimizer, device, num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzgLbvKW_5nS",
        "outputId": "369cac26-a721-4eec-f632-781338389ded"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:28<00:00,  2.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "\tTrain Loss: 0.485\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:27<00:00,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2\n",
            "\tTrain Loss: 0.223\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:25<00:00,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3\n",
            "\tTrain Loss: 0.183\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:26<00:00,  2.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4\n",
            "\tTrain Loss: 0.164\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:26<00:00,  2.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5\n",
            "\tTrain Loss: 0.152\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:26<00:00,  2.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6\n",
            "\tTrain Loss: 0.141\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:25<00:00,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7\n",
            "\tTrain Loss: 0.136\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:26<00:00,  2.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8\n",
            "\tTrain Loss: 0.131\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:26<00:00,  2.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9\n",
            "\tTrain Loss: 0.126\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:24<00:00,  2.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10\n",
            "\tTrain Loss: 0.124\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:25<00:00,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11\n",
            "\tTrain Loss: 0.122\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:27<00:00,  2.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12\n",
            "\tTrain Loss: 0.121\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:30<00:00,  2.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13\n",
            "\tTrain Loss: 0.119\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:32<00:00,  2.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14\n",
            "\tTrain Loss: 0.118\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:27<00:00,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15\n",
            "\tTrain Loss: 0.116\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:26<00:00,  2.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16\n",
            "\tTrain Loss: 0.116\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:24<00:00,  2.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17\n",
            "\tTrain Loss: 0.114\n",
            "Best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:24<00:00,  2.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18\n",
            "\tTrain Loss: 0.388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:22<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19\n",
            "\tTrain Loss: 1.494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 553/553 [03:25<00:00,  2.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20\n",
            "\tTrain Loss: 1.494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def plot_losses(train_losses, test_losses):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    # plt.plot(test_losses, label='Test Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Train Losses over Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "  \n",
        "plot_losses(train_losses, test_losses)"
      ],
      "metadata": {
        "id": "HMs4ByOID0ZT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "275e6715-f5f3-4fb9-e34c-02fce365a57b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAHWCAYAAABACtmGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABcJ0lEQVR4nO3dd3xUVf7/8ffMJJkkpAEhIYFI6CBiRBCkKCAIRH8oiiuWlWJXWGVZv4u4CpZVVCysWBBXQdeGFRuKgCCCKEqxIkVKQgkQSG+TzNzfHyFDQnpI5s4kr+fjMQ9m7j33zmfuXGPeOfeeYzEMwxAAAAAAoFJWswsAAAAAAG9HcAIAAACAahCcAAAAAKAaBCcAAAAAqAbBCQAAAACqQXACAAAAgGoQnAAAAACgGgQnAAAAAKgGwQkAAAAAqkFwAoBGYOLEiYqPjze7DECSZLFYNGXKFLPLAIB6RXACgAZksVhq9Fi9erXZpZaxevVqWSwWvffee2aXggpUdS7deuutZpcHAI2Sn9kFAEBj9r///a/M69dee03Lly8vt7x79+6n9D4vvfSSXC7XKe0DvuXCCy/U+PHjyy3v0qWLCdUAQONHcAKABvTXv/61zOvvvvtOy5cvL7f8ZLm5uQoODq7x+/j7+9epPnin/Px8BQQEyGqt/MKQLl26VHseAQDqD5fqAYDJhgwZojPOOEMbN27U+eefr+DgYN1zzz2SpI8++kgXX3yxYmNjZbfb1bFjRz300ENyOp1l9nHyPU579uyRxWLRE088oQULFqhjx46y2+0655xz9MMPP9Rb7bt27dJf/vIXtWjRQsHBwTr33HP12WeflWs3b9489ejRQ8HBwWrevLn69OmjN998070+KytLU6dOVXx8vOx2u6KionThhRdq06ZNZfbz/fffa9SoUQoPD1dwcLAGDx6sdevWlWlT031VZPPmzUpMTFRYWJhCQkI0bNgwfffdd+71P/74oywWi1599dVy2y5btkwWi0Wffvqpe9n+/ft1/fXXKzo6Wna7XT169NArr7xSZruSyyLffvtt3XvvvWrTpo2Cg4OVmZlZbb3VKX1uDRgwQEFBQWrfvr3mz59fru3hw4d1ww03KDo6WoGBgUpISKjwc7pcLv3nP/9Rz549FRgYqFatWmnUqFH68ccfy7VdsmSJzjjjDPdn/+KLL8qsP5XvCgA8jR4nAPACR48eVWJioq666ir99a9/VXR0tCRp0aJFCgkJ0bRp0xQSEqKvvvpKM2fOVGZmpubMmVPtft98801lZWXplltukcVi0eOPP67LL79cu3btOuVeqkOHDmnAgAHKzc3VHXfcoZYtW+rVV1/VJZdcovfee0+XXXaZpOLLCO+44w5dccUVuvPOO5Wfn6+ff/5Z33//va655hpJ0q233qr33ntPU6ZM0emnn66jR49q7dq12rp1q84++2xJ0ldffaXExET17t1bs2bNktVq1cKFC3XBBRfom2++Ud++fWu8r4r89ttvOu+88xQWFqZ//vOf8vf314svvqghQ4bo66+/Vr9+/dSnTx916NBB77zzjiZMmFBm+8WLF6t58+YaOXKk+/ice+657oESWrVqpc8//1w33HCDMjMzNXXq1DLbP/TQQwoICNBdd92lgoICBQQEVHn88/PzlZqaWm55WFhYmW3T0tJ00UUX6corr9TVV1+td955R7fddpsCAgJ0/fXXS5Ly8vI0ZMgQ7dy5U1OmTFH79u317rvvauLEiUpPT9edd97p3t8NN9ygRYsWKTExUTfeeKOKior0zTff6LvvvlOfPn3c7dauXasPPvhAt99+u0JDQ/XMM89o7NixSkpKUsuWLU/puwIAUxgAAI+ZPHmycfKP3sGDBxuSjPnz55drn5ubW27ZLbfcYgQHBxv5+fnuZRMmTDDatWvnfr17925DktGyZUvj2LFj7uUfffSRIcn45JNPqqxz1apVhiTj3XffrbTN1KlTDUnGN998416WlZVltG/f3oiPjzecTqdhGIZx6aWXGj169Kjy/cLDw43JkydXut7lchmdO3c2Ro4cabhcLvfy3Nxco3379saFF15Y431VZsyYMUZAQIDx559/upcdOHDACA0NNc4//3z3shkzZhj+/v5ljmtBQYERERFhXH/99e5lN9xwgxETE2OkpqaWeZ+rrrrKCA8Pd3+3Jce6Q4cOFX7fFZFU6eOtt95ytys5t5588skytZ511llGVFSU4XA4DMMwjLlz5xqSjNdff93dzuFwGP379zdCQkKMzMxMwzAM46uvvjIkGXfccUe5mkp/L5KMgIAAY+fOne5lP/30kyHJmDdvnntZXb8rADADl+oBgBew2+2aNGlSueVBQUHu51lZWUpNTdV5552n3Nxc/fHHH9Xud9y4cWrevLn79XnnnSep+BK7U7V06VL17dtXgwYNci8LCQnRzTffrD179uj333+XJEVERGjfvn1VXiIYERGh77//XgcOHKhw/ZYtW7Rjxw5dc801Onr0qFJTU5WamqqcnBwNGzZMa9ascQ+OUd2+KuJ0OvXll19qzJgx6tChg3t5TEyMrrnmGq1du9Z96dy4ceNUWFioDz74wN3uyy+/VHp6usaNGydJMgxD77//vkaPHi3DMNz1pqamauTIkcrIyCh3OdqECRPKfN/VufTSS7V8+fJyj6FDh5Zp5+fnp1tuucX9OiAgQLfccosOHz6sjRs3Sir+Llu3bq2rr77a3c7f31933HGHsrOz9fXXX0uS3n//fVksFs2aNatcPRaLpczr4cOHq2PHju7XZ555psLCwsqce3X5rgDALAQnAPACbdq0qfDSrN9++02XXXaZwsPDFRYWplatWrkHBMjIyKh2v6eddlqZ1yUhKi0t7ZRr3rt3r7p27VpueckIgXv37pUkTZ8+XSEhIerbt686d+6syZMnl7sv6fHHH9evv/6quLg49e3bV/fff3+ZX7B37NghqThctGrVqszjv//9rwoKCtzHo7p9VeTIkSPKzc2t9PO4XC4lJydLkhISEtStWzctXrzY3Wbx4sWKjIzUBRdc4N5fenq6FixYUK7ekoB8+PDhMu/Tvn37Kms8Wdu2bTV8+PByj5LLPEvExsaqWbNmZZaVjLy3Z88eScXfVefOncsNRnHyd/nnn38qNjZWLVq0qLa+k889qfj8K33u1eW7AgCzEJwAwAtU1NOQnp6uwYMH66efftKDDz6oTz75RMuXL9djjz0mSTUaftxms1W43DCMUyu4Frp3765t27bp7bff1qBBg/T+++9r0KBBZXotrrzySu3atUvz5s1TbGys5syZox49eujzzz+XdOKzzpkzp8JeluXLlyskJKRG+6oP48aN06pVq5SamqqCggJ9/PHHGjt2rPz8/MrU+9e//rXSegcOHFhmn7XpbfIFNTn3PPFdAUB9YXAIAPBSq1ev1tGjR/XBBx/o/PPPdy/fvXu3iVWd0K5dO23btq3c8pJLCNu1a+de1qxZM40bN07jxo2Tw+HQ5ZdfrocfflgzZsxQYGCgpOLL4m6//XbdfvvtOnz4sM4++2w9/PDDSkxMdF/yFRYWpuHDh1dbW1X7qkirVq0UHBxc6eexWq2Ki4tzLxs3bpweeOABvf/++4qOjlZmZqauuuqqMvsLDQ2V0+msUb0N6cCBA8rJySnT67R9+3ZJco/E2K5dO/38889yuVxlep1O/i47duyoZcuW6dixYzXqdaqJ2n5XAGAWepwAwEuV/MW+9F/oHQ6Hnn/+ebNKKuOiiy7Shg0btH79eveynJwcLViwQPHx8Tr99NMlFY8YWFpAQIBOP/10GYahwsJCOZ3OcpcdRkVFKTY2VgUFBZKk3r17q2PHjnriiSeUnZ1drpYjR45IUo32VRGbzaYRI0boo48+cl++JhWPjPfmm29q0KBBCgsLcy/v3r27evbsqcWLF2vx4sWKiYkpE25tNpvGjh2r999/X7/++mul9XpCUVGRXnzxRfdrh8OhF198Ua1atVLv3r0lFX+XKSkpZS4/LCoq0rx58xQSEqLBgwdLksaOHSvDMPTAAw+Ue5/a9mLW9bsCALPQ4wQAXmrAgAFq3ry5JkyYoDvuuEMWi0X/+9//PHqZ3fvvv1/hIBQTJkzQ3XffrbfeekuJiYm644471KJFC7366qvavXu33n//fXfPxYgRI9S6dWsNHDhQ0dHR2rp1q5599lldfPHFCg0NVXp6utq2basrrrhCCQkJCgkJ0YoVK/TDDz/oySeflCRZrVb997//VWJionr06KFJkyapTZs22r9/v1atWqWwsDB98sknysrKqnZflfn3v/+t5cuXa9CgQbr99tvl5+enF198UQUFBXr88cfLtR83bpxmzpypwMBA3XDDDeXuD3r00Ue1atUq9evXTzfddJNOP/10HTt2TJs2bdKKFSt07Nixun4tkop7jV5//fVyy6Ojo3XhhRe6X8fGxuqxxx7Tnj171KVLFy1evFhbtmzRggUL3EPS33zzzXrxxRc1ceJEbdy4UfHx8Xrvvfe0bt06zZ07V6GhoZKkoUOH6rrrrtMzzzyjHTt2aNSoUXK5XPrmm280dOhQTZkypcb1n8p3BQCmMG08PwBogiobjryy4brXrVtnnHvuuUZQUJARGxtr/POf/zSWLVtmSDJWrVrlblfZcORz5swpt09JxqxZs6qss2SI7MoeJUOQ//nnn8YVV1xhREREGIGBgUbfvn2NTz/9tMy+XnzxReP88883WrZsadjtdqNjx47G//3f/xkZGRmGYRQPj/1///d/RkJCghEaGmo0a9bMSEhIMJ5//vlydW3evNm4/PLL3ftq166dceWVVxorV66s9b4qsmnTJmPkyJFGSEiIERwcbAwdOtT49ttvK2y7Y8cO9/FYu3ZthW0OHTpkTJ482YiLizP8/f2N1q1bG8OGDTMWLFhQ7lhXNfT7yar6bgYPHuxuV3Ju/fjjj0b//v2NwMBAo127dsazzz5bYa2TJk0yIiMjjYCAAKNnz57GwoULy7UrKioy5syZY3Tr1s0ICAgwWrVqZSQmJhobN24sU19Fw4y3a9fOmDBhgmEYp/5dAYCnWQzDg3+6BAAAHjNkyBClpqZWeLkgAKB2uMcJAAAAAKpBcAIAAACAahCcAAAAAKAa3OMEAAAAANWgxwkAAAAAqkFwAgAAAIBqNLkJcF0ulw4cOKDQ0FBZLBazywEAAABgEsMwlJWVpdjY2HITmZ+syQWnAwcOKC4uzuwyAAAAAHiJ5ORktW3btso2TS44hYaGSio+OGFhYSZXAwAAAMAsmZmZiouLc2eEqjS54FRyeV5YWBjBCQAAAECNbuFhcAgAAAAAqAbBCQAAAACqQXACAAAAgGo0uXucasIwDBUVFcnpdJpdCryUzWaTn58fQ9oDAAA0EQSnkzgcDh08eFC5ublmlwIvFxwcrJiYGAUEBJhdCgAAABoYwakUl8ul3bt3y2azKTY2VgEBAfQooBzDMORwOHTkyBHt3r1bnTt3rnbCNAAAAPg2glMpDodDLpdLcXFxCg4ONrsceLGgoCD5+/tr7969cjgcCgwMNLskAAAANCD+TF4Beg9QE5wnAAAATQe/+QEAAABANQhOAAAAAFANghMqFR8fr7lz59a4/erVq2WxWJSent5gNQEAAABmIDg1AhaLpcrH/fffX6f9/vDDD7r55ptr3H7AgAE6ePCgwsPD6/R+NUVAAwAAgKcxql4jcPDgQffzxYsXa+bMmdq2bZt7WUhIiPu5YRhyOp3y86v+q2/VqlWt6ggICFDr1q1rtQ0AAADgCwhO1TAMQ3mFTlPeO8jfVqN5pEqHlfDwcFksFvey1atXa+jQoVq6dKnuvfde/fLLL/ryyy8VFxenadOm6bvvvlNOTo66d++u2bNna/jw4e59xcfHa+rUqZo6daqk4p6tl156SZ999pmWLVumNm3a6Mknn9Qll1xS5r3S0tIUERGhRYsWaerUqVq8eLGmTp2q5ORkDRo0SAsXLlRMTIwkqaioSNOmTdNrr70mm82mG2+8USkpKcrIyNCSJUvqdNzS0tJ055136pNPPlFBQYEGDx6sZ555Rp07d5Yk7d27V1OmTNHatWvlcDgUHx+vOXPm6KKLLlJaWpqmTJmiL7/8UtnZ2Wrbtq3uueceTZo0qU61AAAAmOmp5dv1455jMozi14aKnxiGjj+T+0lF64zjG554XXq7E+sq2n9l+yrx1k3nqnmzgFP5eB5FcKpGXqFTp89cZsp7//7gSAUH1M9XdPfdd+uJJ55Qhw4d1Lx5cyUnJ+uiiy7Sww8/LLvdrtdee02jR4/Wtm3bdNppp1W6nwceeECPP/645syZo3nz5unaa6/V3r171aJFiwrb5+bm6oknntD//vc/Wa1W/fWvf9Vdd92lN954Q5L02GOP6Y033tDChQvVvXt3/ec//9GSJUs0dOjQOn/WiRMnaseOHfr4448VFham6dOn66KLLtLvv/8uf39/TZ48WQ6HQ2vWrFGzZs30+++/u3vl7rvvPv3+++/6/PPPFRkZqZ07dyovL6/OtQAAAJglJSNfz6zcYXYZlSpyGdU38iIEpybiwQcf1IUXXuh+3aJFCyUkJLhfP/TQQ/rwww/18ccfa8qUKZXuZ+LEibr66qslSY888oieeeYZbdiwQaNGjaqwfWFhoebPn6+OHTtKkqZMmaIHH3zQvX7evHmaMWOGLrvsMknSs88+q6VLl9b5c5YEpnXr1mnAgAGSpDfeeENxcXFasmSJ/vKXvygpKUljx45Vz549JUkdOnRwb5+UlKRevXqpT58+kop73QAAAHzR3qM5kqToMLv+dfHpkqSSa5ksFsly/FXJBU6l16mCdSVXQpXZh3v9iYYn1lfQvtR+w4J8K4r4VrUmCPK36fcHR5r23vWlJAiUyM7O1v3336/PPvtMBw8eVFFRkfLy8pSUlFTlfs4880z382bNmiksLEyHDx+utH1wcLA7NElSTEyMu31GRoYOHTqkvn37utfbbDb17t1bLperVp+vxNatW+Xn56d+/fq5l7Vs2VJdu3bV1q1bJUl33HGHbrvtNn355ZcaPny4xo4d6/5ct912m8aOHatNmzZpxIgRGjNmjDuAAQAA+JLktOKrZjpHheqShFiTq/F9jKpXDYvFouAAP1MeNbm/qaaaNWtW5vVdd92lDz/8UI888oi++eYbbdmyRT179pTD4ahyP/7+/uWOT1Uhp6L2J1/f6mk33nijdu3apeuuu06//PKL+vTpo3nz5kmSEhMTtXfvXv3973/XgQMHNGzYMN11112m1gsAAFAXScdyJUlxLYJNrqRxIDg1UevWrdPEiRN12WWXqWfPnmrdurX27Nnj0RrCw8MVHR2tH374wb3M6XRq06ZNdd5n9+7dVVRUpO+//9697OjRo9q2bZtOP/1097K4uDjdeuut+uCDD/SPf/xDL730kntdq1atNGHCBL3++uuaO3euFixYUOd6AAAAzJJ8PDidRnCqF1yq10R17txZH3zwgUaPHi2LxaL77ruvzpfHnYq//e1vmj17tjp16qRu3bpp3rx5SktLq1Fv2y+//KLQ0FD3a4vFooSEBF166aW66aab9OKLLyo0NFR333232rRpo0svvVSSNHXqVCUmJqpLly5KS0vTqlWr1L17d0nSzJkz1bt3b/Xo0UMFBQX69NNP3esAAAB8SRLBqV4RnJqop556Stdff70GDBigyMhITZ8+XZmZmR6vY/r06UpJSdH48eNls9l08803a+TIkbLZqr+/6/zzzy/z2mazqaioSAsXLtSdd96p//f//p8cDofOP/98LV261H3ZoNPp1OTJk7Vv3z6FhYVp1KhRevrppyUVz0U1Y8YM7dmzR0FBQTrvvPP09ttv1/8HBwAAaGAEp/plMcy+4cTDMjMzFR4eroyMDIWFhZVZl5+fr927d6t9+/YKDAw0qcKmzeVyqXv37rryyiv10EMPmV1OlThfAACAt8pzONV95heSpJ9mjlB4sH81WzRNVWWDk9HjBFPt3btXX375pQYPHqyCggI9++yz2r17t6655hqzSwMAAPBZ+9KKe5tCA/0ITfWEwSFgKqvVqkWLFumcc87RwIED9csvv2jFihXcVwQAAHAKuEyv/tHjBFPFxcVp3bp1ZpcBAADQqBCc6h89TgAAAEAjQ3CqfwSnCjSx8TJQR5wnAADAWyUz+W29IziVUjJcdW5ursmVwBeUnCcl5w0AAIC3oMep/nGPUyk2m00RERE6fPiwJCk4OLhGE7GiaTEMQ7m5uTp8+LAiIiJqNOcUAACApxiGoeRjeZIITvWJ4HSS1q1bS5I7PAGViYiIcJ8vAAAA3iI126G8QqcsFik2IsjschoNgtNJLBaLYmJiFBUVpcLCQrPLgZfy9/enpwkAAHilksv0YsODFODHnTn1heBUCZvNxi/GAAAA8DknBoagt6k+EUEBAACARoSBIRoGwQkAAABoRAhODcPU4LRmzRqNHj1asbGxslgsWrJkSY23Xbdunfz8/HTWWWc1WH0AAACAr0liDqcGYWpwysnJUUJCgp577rlabZeenq7x48dr2LBhDVQZAAAA4Jv2EZwahKmDQyQmJioxMbHW291666265pprZLPZatVLBQAAADRmBUVOHczMl8SlevXN5+5xWrhwoXbt2qVZs2bVqH1BQYEyMzPLPAAAAIDGaH9angxDCg6wqWWzALPLaVR8Kjjt2LFDd999t15//XX5+dWss2z27NkKDw93P+Li4hq4SgAAAMAcpQeGsFgsJlfTuPhMcHI6nbrmmmv0wAMPqEuXLjXebsaMGcrIyHA/kpOTG7BKAAAAwDzJ3N/UYHxmAtysrCz9+OOP2rx5s6ZMmSJJcrlcMgxDfn5++vLLL3XBBReU285ut8tut3u6XAAAAMDjGIq84fhMcAoLC9Mvv/xSZtnzzz+vr776Su+9957at29vUmUAAACAdyA4NRxTg1N2drZ27tzpfr17925t2bJFLVq00GmnnaYZM2Zo//79eu2112S1WnXGGWeU2T4qKkqBgYHllgMAAABNUfKxPElSXIsgkytpfEwNTj/++KOGDh3qfj1t2jRJ0oQJE7Ro0SIdPHhQSUlJZpUHAAAA+AzDMNz3ONHjVP8shmEYZhfhSZmZmQoPD1dGRobCwsLMLgcAAACoF2k5DvV6aLkk6Y+HRinQ32ZyRd6vNtnAZ0bVAwAAAFC5kvubosPshKYGQHACAAAAGgEGhmhYBCcAAACgEUhiDqcGRXACAAAAGgH35LfNCU4NgeAEAAAANALJaVyq15AITgAAAEAj4L7HqSXBqSEQnAAAAAAfV+h06UB6viR6nBoKwQkAAADwcQfT8+V0GbL7WdUqxG52OY0SwQkAAADwcaVH1LNaLSZX0zgRnAAAAAAfxxxODY/gBAAAAPi4khH14poHmVxJ40VwAgAAAHwck982PIITAAAA4OOSuVSvwRGcAAAAAB/HHE4Nj+AEAAAA+LCMvEKl5xZKkuKaE5waCsEJAAAA8GEll+lFhgSomd3P5GoaL4ITAAAA4MNKglNbepsaFMEJAAAA8GElQ5EzMETDIjgBAAAAPozJbz2D4AQAAAD4sKRjeZIITg2N4AQAAAD4sGQmv/UIghMAAADgo5wuQ/vSmMPJEwhOAAAAgI9KycxXodOQv82i1mGBZpfTqBGcAAAAAB9Vcplem4gg2awWk6tp3AhOAAAAgI9K4v4mjyE4AQAAAD4qmaHIPYbgBAAAAPgo5nDyHIITAAAA4KMITp5DcAIAAAB8FHM4eQ7BCQAAAPBBOQVFSs12SCI4eQLBCQAAAPBB+9LyJEnhQf4KD/I3uZrGj+AEAAAA+CDub/IsghMAAADggwhOnkVwAgAAAHwQA0N4FsEJAAAA8EH0OHkWwQkAAADwQQQnzyI4AQAAAD7GMIxSl+oFmVxN00BwAgAAAHzMkawCFRS5ZLVIsREEJ08gOAEAAAA+puQyvdiIIPnb+JXeEzjKAAAAgI/h/ibPMzU4rVmzRqNHj1ZsbKwsFouWLFlSZfsPPvhAF154oVq1aqWwsDD1799fy5Yt80yxAAAAgJcgOHmeqcEpJydHCQkJeu6552rUfs2aNbrwwgu1dOlSbdy4UUOHDtXo0aO1efPmBq4UAAAA8B5JzOHkcX5mvnliYqISExNr3H7u3LllXj/yyCP66KOP9Mknn6hXr171XB0AAADgnZj81vNMDU6nyuVyKSsrSy1atKi0TUFBgQoKCtyvMzMzPVEaAAAA0GCSj+VJ4lI9T/LpwSGeeOIJZWdn68orr6y0zezZsxUeHu5+xMXFebBCAAAAoH7lFzqVkpkvieDkST4bnN5880098MADeueddxQVFVVpuxkzZigjI8P9SE5O9mCVAAAAQP3al1bc2xRi91PzYH+Tq2k6fPJSvbfffls33nij3n33XQ0fPrzKtna7XXa73UOVAQAAAA2r9P1NFovF5GqaDp/rcXrrrbc0adIkvfXWW7r44ovNLgcAAADwqBNDkQeZXEnTYmqPU3Z2tnbu3Ol+vXv3bm3ZskUtWrTQaaedphkzZmj//v167bXXJBVfnjdhwgT95z//Ub9+/ZSSkiJJCgoKUnh4uCmfAQAAAPAk5nAyh6k9Tj/++KN69erlHkp82rRp6tWrl2bOnClJOnjwoJKSktztFyxYoKKiIk2ePFkxMTHux5133mlK/QAAAICnMYeTOUztcRoyZIgMw6h0/aJFi8q8Xr16dcMWBAAAAHg55nAyh8/d4wQAAAA0VYZhuIMTl+p5FsEJAAAA8BHHchzKcThlsUhtIhgcwpMITgAAAICPKLm/qXVYoAL9bSZX07QQnAAAAAAfwcAQ5iE4AQAAAD7CPTBEc4KTpxGcAAAAAB+RfCxPEgNDmIHgBAAAAPgI9+S3LRkYwtMITgAAAICPSGIoctMQnAAAAAAf4Chy6WBG8aV6DA7heQQnAAAAwAccSM+Ty5AC/a1qFWI3u5wmh+AEAAAA+ICkUiPqWSwWk6tpeghOAAAAgA/g/iZzEZwAAAAAH5CcxuS3ZiI4AQAAAD4gmR4nUxGcAAAAAB/ApXrmIjgBAAAAPiDpaMnktwQnMxCcAAAAAC+XkVuozPwiScWj6sHzCE4AAACAlyu5TC8yxK6gAJvJ1TRNBCcAAADAy5WMqHdaiyCTK2m6CE4AAACAl2NgCPMRnAAAAAAvR3AyH8EJAAAA8HIlczgx+a15CE4AAACAl6PHyXwEJwAAAMCLFTld2p+WJ4keJzMRnAAAAAAvdjAjX0UuQwE2q6LDAs0up8kiOAEAAABerGQo8rbNg2SzWkyupukiOAEAAABejIEhvAPBCQAAAPBiDAzhHQhOAAAAgBdLOlY8MATByVwEJwAAAMCLJXGpnlcgOAEAAABe7MQ9TkEmV9K0EZwAAAAAL5VdUKRjOQ5J9DiZjeAEAAAAeKmS3qbmwf4KC/Q3uZqmjeAEAAAAeClG1PMeBCcAAADASzGHk/cgOAEAAABeih4n70FwAgAAALwUQ5F7D4ITAAAA4KXocfIeBCcAAADAC7lchval5UkiOHkDghMAAADghQ5nFchR5JLNalFMeKDZ5TR5pganNWvWaPTo0YqNjZXFYtGSJUuq3Wb16tU6++yzZbfb1alTJy1atKjB6wQAAAA8reQyvTYRQfKz0d9hNlO/gZycHCUkJOi5556rUfvdu3fr4osv1tChQ7VlyxZNnTpVN954o5YtW9bAlQIAAACexf1N3sXPzDdPTExUYmJijdvPnz9f7du315NPPilJ6t69u9auXaunn35aI0eObKgyAQAAAI9jRD3v4lN9fuvXr9fw4cPLLBs5cqTWr19f6TYFBQXKzMws8wAAAAC83YnJb4NMrgSSjwWnlJQURUdHl1kWHR2tzMxM5eXlVbjN7NmzFR4e7n7ExcV5olQAAADglHCpnnfxqeBUFzNmzFBGRob7kZycbHZJAAAAQLWSCU5exdR7nGqrdevWOnToUJllhw4dUlhYmIKCKu7CtNvtstvtnigPAAAAqBd5DqcOZxVIIjh5C5/qcerfv79WrlxZZtny5cvVv39/kyoCAAAA6t++tOLeptBAP4UH+ZtcDSSTg1N2dra2bNmiLVu2SCoebnzLli1KSkqSVHyZ3fjx493tb731Vu3atUv//Oc/9ccff+j555/XO++8o7///e9mlA8AAAA0iNL3N1ksFpOrgWRycPrxxx/Vq1cv9erVS5I0bdo09erVSzNnzpQkHTx40B2iJKl9+/b67LPPtHz5ciUkJOjJJ5/Uf//7X4YiBwAAQKPiHoq8OZfpeQtT73EaMmSIDMOodP2iRYsq3Gbz5s0NWBUAAABgLnePU0uCk7fwqXucAAAAgKYg+VjxVDtMfus9CE4AAACAl2Eocu9DcAIAAAC8iGEYTH7rhQhOAAAAgBdJzXYor9Api0VqE1HxXKXwPIITAAAA4EVKeptiw4MU4Mev696CbwIAAADwIiX3N7VtTm+TNyE4AQAAAF6E+5u8E8EJAAAA8CKMqOedCE4AAACAF2HyW+9EcAIAAAC8SEmPE5PfeheCEwAAAOAlCoqcOpiZL4lL9bwNwQkAAADwEvvT8mQYUpC/TS2bBZhdDkohOAEAAABeovSIehaLxeRqUBrBCQAAAPAS3N/kvQhOAAAAgJdITsuTxP1N3ojgBAAAAHiJpKMll+oFmVwJTkZwAgAAALwEczh5L4ITAAAA4AUMw3Df48Slet6H4AQAAAB4gfTcQmUVFEmS2jYnOHkbghMAAADgBUou04sKtSvQ32ZyNTgZwQkAAADwAslpXKbnzQhOAAAAgBdI4v4mr0ZwAgAAALwAk996N4ITAAAA4AXocfJuBCcAAADACyTR4+TVCE4AAACAyQqdLh1Iz5dEj5O3IjgBAAAAJjuYni+ny1CAn1VRoXazy0EFCE4AAACAyUqGIo9rHiSr1WJyNagIwQkAAAAwGQNDeD+CEwAAAGAygpP3IzgBAAAAJmNEPe9Xp+CUnJysffv2uV9v2LBBU6dO1YIFC+qtMAAAAKCpYPJb71en4HTNNddo1apVkqSUlBRdeOGF2rBhg/71r3/pwQcfrNcCAQAAgMaOS/W8X52C06+//qq+fftKkt555x2dccYZ+vbbb/XGG29o0aJF9VkfAAAA0Khl5hcqPbdQEj1O3qxOwamwsFB2e/H48itWrNAll1wiSerWrZsOHjxYf9UBAAAAjVzJZXotmwUoxO5ncjWoTJ2CU48ePTR//nx98803Wr58uUaNGiVJOnDggFq2bFmvBQIAAACNGfc3+YY6BafHHntML774ooYMGaKrr75aCQkJkqSPP/7YfQkfAAAAgOpxf5NvqFNf4JAhQ5SamqrMzEw1b97cvfzmm29WcDBfOAAAAFBTJ4YiDzK5ElSlTj1OeXl5KigocIemvXv3au7cudq2bZuioqLqtUAAAACgMUs6lieJHidvV6fgdOmll+q1116TJKWnp6tfv3568sknNWbMGL3wwgu12tdzzz2n+Ph4BQYGql+/ftqwYUOV7efOnauuXbsqKChIcXFx+vvf/678/Py6fAwAAADAdNzj5BvqFJw2bdqk8847T5L03nvvKTo6Wnv37tVrr72mZ555psb7Wbx4saZNm6ZZs2Zp06ZNSkhI0MiRI3X48OEK27/55pu6++67NWvWLG3dulUvv/yyFi9erHvuuacuHwMAAAAwldNlaH8aPU6+oE7BKTc3V6GhoZKkL7/8UpdffrmsVqvOPfdc7d27t8b7eeqpp3TTTTdp0qRJOv300zV//nwFBwfrlVdeqbD9t99+q4EDB+qaa65RfHy8RowYoauvvrraXioAAADAGx3KzJfD6ZKf1aKYcO5x8mZ1Ck6dOnXSkiVLlJycrGXLlmnEiBGSpMOHDyssLKxG+3A4HNq4caOGDx9+ohirVcOHD9f69esr3GbAgAHauHGjOyjt2rVLS5cu1UUXXVTp+xQUFCgzM7PMAwAAAPAGJQNDtG0eJJvVYnI1qEqdgtPMmTN11113KT4+Xn379lX//v0lFfc+9erVq0b7SE1NldPpVHR0dJnl0dHRSklJqXCba665Rg8++KAGDRokf39/dezYUUOGDKnyUr3Zs2crPDzc/YiLi6vhpwQAAAAaVhL3N/mMOgWnK664QklJSfrxxx+1bNky9/Jhw4bp6aefrrfiTrZ69Wo98sgjev7557Vp0yZ98MEH+uyzz/TQQw9Vus2MGTOUkZHhfiQnJzdYfQAAAEBtMDCE76jTPE6S1Lp1a7Vu3Vr79u2TJLVt27ZWk99GRkbKZrPp0KFDZZYfOnRIrVu3rnCb++67T9ddd51uvPFGSVLPnj2Vk5Ojm2++Wf/6179ktZbPgXa7XXa7vcZ1AQAAAJ7C5Le+o049Ti6XSw8++KDCw8PVrl07tWvXThEREXrooYfkcrlqtI+AgAD17t1bK1euLLPflStXui/9O1lubm65cGSz2SRJhmHU5aMAAAAApiE4+Y469Tj961//0ssvv6xHH31UAwcOlCStXbtW999/v/Lz8/Xwww/XaD/Tpk3ThAkT1KdPH/Xt21dz585VTk6OJk2aJEkaP3682rRpo9mzZ0uSRo8eraeeekq9evVSv379tHPnTt13330aPXq0O0ABAAAAviKZyW99Rp2C06uvvqr//ve/uuSSS9zLzjzzTLVp00a33357jYPTuHHjdOTIEc2cOVMpKSk666yz9MUXX7gHjEhKSirTw3TvvffKYrHo3nvv1f79+9WqVSuNHj26xu8HAAAAeItcR5FSswskcY+TL7AYdbjGLTAwUD///LO6dOlSZvm2bdt01llnKS8vr94KrG+ZmZkKDw9XRkZGjYdOBwAAAOrbtpQsjZy7RuFB/vpp1gizy2mSapMN6nSPU0JCgp599tlyy5999lmdeeaZddklAAAA0KScGIqciW99QZ0u1Xv88cd18cUXa8WKFe6BHNavX6/k5GQtXbq0XgsEAAAAGiMGhvAtdepxGjx4sLZv367LLrtM6enpSk9P1+WXX67ffvtN//vf/+q7RgAAAKDRYQ4n31LneZxiY2PLDcrw008/6eWXX9aCBQtOuTAAAACgMUumx8mn1KnHCQAAAMCp4VI930JwAgAAADzMMAyCk48hOAEAAAAediSrQAVFLlktUmwEo+r5glrd43T55ZdXuT49Pf1UagEAAACahJLeppjwIPnb6MvwBbUKTuHh4dWuHz9+/CkVBAAAADR2XKbne2oVnBYuXNhQdQAAAABNBsHJ99AvCAAAAHhY8rE8SdJpLQlOvoLgBAAAAHgYk9/6HoITAAAA4GFcqud7CE4AAACAB+UXOpWSmS9JimvOUOS+guAEAAAAeNC+tOL7m5oF2NSiWYDJ1aCmCE4AAACAB5W+v8lisZhcDWqK4AQAAAB4UHIa9zf5IoITAAAA4EFJRwlOvojgBAAAAHiQe0Q95nDyKQQnAAAAwINKglNcc4KTLyE4AQAAAB5iGAaT3/ooghMAAADgIcdyHMpxOCVJbZnDyacQnAAAAAAPKblMr3VYoAL9bSZXg9ogOAEAAAAeknx88ltG1PM9BCcAAADAQ7i/yXcRnAAAAAAPYQ4n30VwAgAAADzEPRR5CwaG8DUEJwAAAMBD3JPf0uPkcwhOAAAAgAcUOl06mMHgEL6K4AQAAAB4wIH0PLkMye5nVatQu9nloJYITgAAAIAHlL5Mz2KxmFwNaovgBAAAAHgA9zf5NoITAAAA4AFJzOHk0whOAAAAgAcw+a1vIzgBAAAAHsCler6N4AQAAAB4QNJRgpMvIzgBAAAADSwjt1CZ+UWSpLgWQSZXg7ogOAEAAAANLDmtuLcpMsSu4AA/k6tBXRCcAAAAgAZ24v4mept8FcEJAAAAaGAMRe77TA9Ozz33nOLj4xUYGKh+/fppw4YNVbZPT0/X5MmTFRMTI7vdri5dumjp0qUeqhYAAACoPUbU832mXmC5ePFiTZs2TfPnz1e/fv00d+5cjRw5Utu2bVNUVFS59g6HQxdeeKGioqL03nvvqU2bNtq7d68iIiI8XzwAAABQQ8zh5PtMDU5PPfWUbrrpJk2aNEmSNH/+fH322Wd65ZVXdPfdd5dr/8orr+jYsWP69ttv5e/vL0mKj4/3ZMkAAABArdHj5PtMu1TP4XBo48aNGj58+IlirFYNHz5c69evr3Cbjz/+WP3799fkyZMVHR2tM844Q4888oicTmel71NQUKDMzMwyDwAAAMBTnC5D+9PyJBGcfJlpwSk1NVVOp1PR0dFllkdHRyslJaXCbXbt2qX33ntPTqdTS5cu1X333acnn3xS//73vyt9n9mzZys8PNz9iIuLq9fPAQAAAFTlYEaeilyGAmxWRYcFml0O6sj0wSFqw+VyKSoqSgsWLFDv3r01btw4/etf/9L8+fMr3WbGjBnKyMhwP5KTkz1YMQAAAJq6ksv02jQPks1qMbka1JVp9zhFRkbKZrPp0KFDZZYfOnRIrVu3rnCbmJgY+fv7y2azuZd1795dKSkpcjgcCggIKLeN3W6X3W6v3+IBAACAGmJgiMbBtB6ngIAA9e7dWytXrnQvc7lcWrlypfr371/hNgMHDtTOnTvlcrncy7Zv366YmJgKQxMAAABgNia/bRxMvVRv2rRpeumll/Tqq69q69atuu2225STk+MeZW/8+PGaMWOGu/1tt92mY8eO6c4779T27dv12Wef6ZFHHtHkyZPN+ggAAABAlZKOMTBEY2DqcOTjxo3TkSNHNHPmTKWkpOiss87SF1984R4wIikpSVbriWwXFxenZcuW6e9//7vOPPNMtWnTRnfeeaemT59u1kcAAAAAqpTMUOSNgsUwDMPsIjwpMzNT4eHhysjIUFhYmNnlAAAAoJHr/dByHc1x6LM7BqlHbLjZ5aCU2mQDnxpVDwAAAPAl2QVFOprjkMTgEL6O4AQAAAA0kJLL9CKC/RUW6G9yNTgVBCcAAACggSRxf1OjQXACAAAAGghzODUeBCcAAACggdDj1HgQnAAAAIAGwlDkjQfBCQAAAGgg9Dg1HgQnAAAAoAG4XIaS0/IkSXHNCU6+juAEAAAANIDDWQVyFLlks1oUExFodjk4RQQnAAAAoAGUXKYXGxEofxu/dvs6vkEAAACgAXB/U+NCcAIAAAAaACPqNS4EJwAAAKABMPlt40JwAgAAABoAl+o1LgQnAAAAoAGUBCeGIm8cCE4AAABAPctzOHU4q0ASPU6NBcEJAAAAqGf70op7m0LtfooI9je5GtQHghMAAABQz5JKDQxhsVhMrgb1geAEAAAA1DOGIm98CE4AAABAPUs6lidJOq0lwamxIDiZ7EhWgQzDMLsMAAAA1KMTI+oFmVwJ6gvByUQvfv2nzn98lT79+aDZpQAAAKAeMflt40NwMlF+oUt5hU499sUfyi90ml0OAAAA6oFhGEx+2wgRnEx00/ntFR1m1760PL367R6zywEAAEA9SM12KK/QKYtFasOleo0GwclEwQF+umtEV0nSs6t26liOw+SKAAAAcKpKeptiwgJl97OZXA3qC8HJZGPPbqvTY8KUlV+k/6zYbnY5AAAAOEUlk99yf1PjQnAymdVq0b0Xd5ckvfF9kv48km1yRQAAADgVSUe5v6kxIjh5gQGdIjWsW5SKXIZmL/3D7HIAAABwCpIYUa9RIjh5iRkXdZfNatGKrYe0/s+jZpcDAACAOmJEvcaJ4OQlOkWF6Jq+p0mSHl76u1wuJsUFAADwRczh1DgRnLzI1OGdFWr306/7M/Xh5v1mlwMAAIBaKihy6mBmviR6nBobgpMXaRli1+1DO0mS5izbpjwHk+ICAAD4kgPp+TIMKcjfpsiQALPLQT0iOHmZSQPj1SYiSCmZ+frvN7vMLgcAAAC1UPr+JovFYnI1qE8EJy8T6G/TP0cVT4r7wtd/6nBWvskVAQAAoKZOjKgXZHIlqG8EJy90SUKsEuIilOtw6unlTIoLAADgKxgYovEiOHkhi8Wi+45Pirv4h2RtS8kyuSIAAADUBJPfNl4EJy/VJ76FEs9oLZchPbx0q9nlAAAAoAaYw6nxIjh5sbsTu8nfZtGa7Ue0etths8sBAABAFQzDcF+qR3BqfAhOXqxdy2Ya3z9ekvTI0q0qcrrMLQgAAACVysgrVFZBkSSpbXOCU2NDcPJyf7ugk8KD/LX9ULbe+XGf2eUAAACgEiWX6bUKtSsowGZyNahvXhGcnnvuOcXHxyswMFD9+vXThg0barTd22+/LYvFojFjxjRsgSaKCA7QHcM6S5KeWr5N2cf/igEAAADvwv1NjZvpwWnx4sWaNm2aZs2apU2bNikhIUEjR47U4cNV39OzZ88e3XXXXTrvvPM8VKl5rju3neJbBis126H5q/80uxwAAABUgODUuJkenJ566inddNNNmjRpkk4//XTNnz9fwcHBeuWVVyrdxul06tprr9UDDzygDh06eLBacwT4WXV3YjdJ0kvf7NKB9DyTKwIAAMDJmMOpcTM1ODkcDm3cuFHDhw93L7NarRo+fLjWr19f6XYPPvigoqKidMMNN1T7HgUFBcrMzCzz8EUje7RW3/gWKihy6Yll28wuBwAAACehx6lxMzU4paamyul0Kjo6uszy6OhopaSkVLjN2rVr9fLLL+ull16q0XvMnj1b4eHh7kdcXNwp120Gi8Wifx2fFPeDzfv1y74MkysCAABAacnHiq8KIjg1TqZfqlcbWVlZuu666/TSSy8pMjKyRtvMmDFDGRkZ7kdycnIDV9lwEuIidOlZsZKkf3/2uwzDMLkiAAAASFKR06X96QSnxszPzDePjIyUzWbToUOHyiw/dOiQWrduXa79n3/+qT179mj06NHuZS5X8dxGfn5+2rZtmzp27FhmG7vdLrvd3gDVm+P/RnbV57+m6Pvdx7T890Ma0aP8cQIAAIBn/ZGSJafLUICfVVGhjed3T5xgao9TQECAevfurZUrV7qXuVwurVy5Uv379y/Xvlu3bvrll1+0ZcsW9+OSSy7R0KFDtWXLFp+9DK822jYP1g2D2kuSHv38DxUyKS4AAIDpnlm5Q5J0QdcoWa0Wk6tBQzC1x0mSpk2bpgkTJqhPnz7q27ev5s6dq5ycHE2aNEmSNH78eLVp00azZ89WYGCgzjjjjDLbR0RESFK55Y3Z7UM66p0fkrUrNUdvfLdXEwe2N7skAACAJmvj3jR9+fshWS3SXSO7mF0OGojpwWncuHE6cuSIZs6cqZSUFJ111ln64osv3ANGJCUlyWr1qVuxGlxooL+mXthF9y35Vf9ZuUOXnd1W4UH+ZpcFAADQ5BiGoce++EOSdEXvtuoUFWpyRWgoFqOJjTCQmZmp8PBwZWRkKCwszOxy6qzI6dKo/3yjnYezdfP5HXTPRd3NLgkAAKDJWbXtsCYt/EEBflatvmuIYiOCzC4JtVCbbEBXjo/ys1l1z0XFk+IuWrfHPeEaAAAAPMPlMvT4F8Xza07o347Q1MgRnHzY0K5RGtippRxOlx493kUMAAAAz/jk5wPaejBToXY/3T6kk9nloIERnHyYxWLRvy46XRaL9NnPB7Vxb5rZJQEAADQJjiKXnvxyuyTplsEd1LxZgMkVoaERnHzc6bFh+kvvtpKYFBcAAMBT3tqQpKRjuWoVatf1gxjhuCkgODUC/xjRVUH+Nm1OStdnvxw0uxwAAIBGLaegSPO+Kp636Y5hnRUcYPpA1fAAglMjEB0WqFsGd5AkPfbFHyoocppcEQAAQOP18trdSs12qF3LYF11TpzZ5cBDCE6NxM3nd1B0mF3Jx/L06rd7zC4HAACgUTqaXaAFa3ZJKr7qx9/Gr9NNBd90IxEc4Kd/jOgqSZr31U4dy3GYXBEAAEDj89yqP5VdUKQesWH6fz1jzC4HHkRwakTGnt1W3WPClJVfpGdW7jC7HAAAgEZlX1quXv9uryRp+qhuslotJlcETyI4NSI2q0X3XtxdkvT6d3u160i2yRUBAAA0Hk8v3yGH06X+HVrqvM6RZpcDDyM4NTIDO0Xqgm5RKnIZmv05k+ICAADUh20pWfpg8z5J0vTEbrJY6G1qaghOjdA9F3WTzWrR8t8P6btdR80uBwAAwOfNWbZNhiElntFaZ8VFmF0OTEBwaoQ6RYXq6r7FQ2P++7Pf5XIxKS4AAEBd/bjnmFZsPSSrRe7BuND0EJwaqanDuyjE7qdf92dqyZb9ZpcDAADgkwzD0GNfFN/+cGWfOHWKCjG5IpiF4NRIRYbYdfvQjpKKu5bzHEyKCwAAUFurth3WD3vSZPez6s7hnc0uByYiODVi1w9srzYRQTqYka+X1+4yuxwAAACf4nQZevyLbZKkiQPiFRMeZHJFMBPBqREL9Lfpn6OKr8N9YfWfOpyVb3JFAAAAvuOjLfv1R0qWwgL9dNuQjmaXA5MRnBq50WfGKqFtuHIcTj29nElxAQAAaqKgyKmnlm+XJN06pKMiggNMrghmIzg1clarRff+v9MlSYt/SNK2lCyTKwIAAPB+b36fpH1peYoKtWvSgPZmlwMvQHBqAs6Jb6FRPVrLZUiPLN1qdjkAAABeLbugSM9+tVOSdOfwzgoKsJlcEbwBwamJuDuxm/xtFn29/YjWbD9idjkAAABe67/f7NLRHIfaRzbTlX3izC4HXoLg1ETERzbTdefGSyrudXIyKS4AAEA5qdkFemlN8WjEd43oKn8bvy6jGGdCE3LHsE4KD/LXHylZevfHZLPLAQAA8DrPfrVTOQ6nerYJV+IZrc0uB16E4NSERAQH6G8XdJIkPbl8u3IKikyuCAAAwHskH8vVG9/vlSRNH9VNVqvF5IrgTQhOTcz4/vFq1zJYR7IK9OLXf5pdDgAAgNd4evl2FToNDeoUqUGdI80uB16G4NTEBPhZdfeobpKkBd/s0sGMPJMrAgAAMN/Wg5n6cMt+SdI/R3U1uRp4I4JTEzTqjNY6J7658gtdemLZdrPLAQAAMN2cZdtkGNLFPWN0ZtsIs8uBFyI4NUEWi0X/urh4UtwPNu/Tr/szTK4IAADAPBt2H9NXfxyWzWrRP0Z0MbsceCmCUxN1VlyELkmIlWFID3+2VYbB8OQAAKDpMQxDj33xhyRp3Dlx6tAqxOSK4K0ITk3YP0d1VYCfVet3HdXKrYfNLgcAAMDjVmw9rI170xTob9WdwzqbXQ68GMGpCWvbPFjXD2wvSXrk860qdLpMrggAAMBznC5Dc5YV9zZNGthe0WGBJlcEb0ZwauJuH9pRLZoFaNeRHL21IcnscgAAADzmw837tf1QtsKD/HXr4I5mlwMvR3Bq4sIC/fX34cXd0nNX7FBmfqHJFQEAADS8/EKnnl5ePLrwbUM6KjzI3+SK4O0ITtDVfU9Tx1bNdCzHoedW7TS7HAAAgAb3xvdJ2p+ep9ZhgZo4IN7scuADCE6Qn82qey7qLklauHaPko/lmlwRAABAw8nKL3T/sXjq8M4K9LeZXBF8AcEJkqQLukVpQMeWcjhd7iE5AQAAGqOX1uzSsRyHOrRqpit6tzW7HPgIghMklUyK210Wi/Tpzwf17o/J3O8EAAAanSNZBfrv2t2SpP8b0VV+Nn4dRs1wpsCtR2y4xp5d/FeX/3vvZ/V6cLn+Mv9bzVu5Qz8lp8vlYpJcAADg2579aodyHU4ltA3XqDNam10OfIif2QXAu9x/SQ9FBPnrqz8Oa1dqjn7Yk6Yf9qTpyeXb1TzYX4M6t9L5nSN1fpdWzHUAAAB8StLRXL15fPqV6aO6yWKxmFwRfInFMIwm1Y2QmZmp8PBwZWRkKCwszOxyvFrysVyt2XFEa7Yf0bc7jyqroKjM+q7RoTq/S3GIOie+BTdWAgAAr3bn25v10ZYDOq9zpP53Qz+zy4EXqE028Irg9Nxzz2nOnDlKSUlRQkKC5s2bp759+1bY9qWXXtJrr72mX3/9VZLUu3dvPfLII5W2PxnBqW4KnS5tSU7Xmu3FQern/RkqfeYE+lvVr31Lnd+luEeqU1QIf8UBAABe47cDGbr4mbWSpE//NkhntAk3uSJ4A58KTosXL9b48eM1f/589evXT3PnztW7776rbdu2KSoqqlz7a6+9VgMHDtSAAQMUGBioxx57TB9++KF+++03tWnTptr3IzjVj2M5Dq3bmVocpHYc0aHMgjLrY8MDdV7nVjq/SysN6hSp8GAmlQMAAOaZuHCDVm87otEJsZp3dS+zy4GX8Kng1K9fP51zzjl69tlnJUkul0txcXH629/+prvvvrva7Z1Op5o3b65nn31W48ePL7e+oKBABQUnfqnPzMxUXFwcwakeGYah7Yey3SHq+93H5ChyuddbLVJCXITOPx6kEtqGM4INAADwmO92HdVVC76Tn9WiFdMGKz6ymdklwUvUJjiZOjiEw+HQxo0bNWPGDPcyq9Wq4cOHa/369TXaR25urgoLC9WiRYsK18+ePVsPPPBAvdSLilksFnVtHaqurUN10/kdlOdwasOeY+7L+nYcztbmpHRtTkrXf1buUFignwZ2Kr436vwurdQmIsjsjwAAABopwzDcc1Re1TeO0IQ6MzU4paamyul0Kjo6uszy6Oho/fFHzSZhnT59umJjYzV8+PAK18+YMUPTpk1zvy7pcULDCQqwaXCXVhrcpZUk6UB6nr7ZcURrtqdq7c5UZeQV6vNfU/T5rymSpI6tmrlD1LntWyoogEEmAABA/fjy90PanJSuIH+b7rigs9nlwIf59HDkjz76qN5++22tXr1agYEVD41tt9tlt9s9XBlKi40I0rhzTtO4c06T02Xop33p+mZ7qtbsOKLNSWn680iO/jySo4Xr9ijAZtU57Zu7L+vr1jqUQSYAAECdFDldmrNsmyTp+kHximIqFZwCU4NTZGSkbDabDh06VGb5oUOH1Lp11ROSPfHEE3r00Ue1YsUKnXnmmQ1ZJuqRzWrR2ac119mnNdedwzsrI69Q3+5MPT7sear2p+dp3c6jWrfzqGZ//oeiQu3HB5mI1OkxYWrTPEjBAT6d9wEAgId8sHm/dh7OVkSwv24Z3NHscuDjTP0NNCAgQL1799bKlSs1ZswYScWDQ6xcuVJTpkypdLvHH39cDz/8sJYtW6Y+ffp4qFo0hPAgfyX2jFFizxgZhqE/j+RozfYj+mbHEX2365gOZxXo/U379P6mfe5tIkMC1LZ5sNo2D1Jci+P/Ng9WXItgxUYEyu7HpX4AADR1+YVOzV2+XZI0eUgnhQUywi9Ojel/up82bZomTJigPn36qG/fvpo7d65ycnI0adIkSdL48ePVpk0bzZ49W5L02GOPaebMmXrzzTcVHx+vlJTi+2RCQkIUEhJi2ufAqbNYLOoUFaJOUSG6flB7FRQ59eOeNK3ZfkTrdx3VntQcZeYXKTXbodRsh7Ykp1ewDyk6NFBxLYLUtnmw4poHqW2pcBUTHsiIfgAANAH/W79XBzLyFRMeqOv6tzO7HDQCpgencePG6ciRI5o5c6ZSUlJ01lln6YsvvnAPGJGUlCSr9cQvui+88IIcDoeuuOKKMvuZNWuW7r//fk+WjgZm97NpYKdIDewU6V6WkVeo5GO52peWp31pue7nyWm5Sj6Wp7xCp1Iy85WSma8f9qSV26fNalFMeKDiSvVYnQhZwYoKtctq5Z4qAAB8WWZ+oZ5bvVOS9PfhXRToz9UoOHWmz+PkaUyA23gZhqFjOQ4lu0NVcaDal5anfccDlsPpqnIfATar2jQPUtvmx8NUi6AyIatlswAGqwAAwMs9sWybnl21U52iQvTFnedxtQkq5TPzOAH1yWKxqGWIXS1D7DorLqLcepfL0JHsAiUfyy0OVKWCVXJarg6k58vhdGl3ao52p+ZU+B5B/rYy91ZFhwUqMiRALZvZ1TIkQJEhdkWG2BlSHQAAkxzOzNfLa3dLkv5vZFdCE+oNwQlNhtVqUXRYoKLDAtUnvvyEyUVOl1Iy88v1VJU8T8nMV16hUzsOZ2vH4ewq3ys4wKbIkOIw1bKZXZHHQ1XLkAC1DLErslmAIkPtatksQBHBAbJxeSAAAPVi3lc7lVfoVK/TIjTi9OjqNwBqiOAEHOdnsx4frS9Y/dWy3PqCIqcOpOe7LwPcl5arI1kFOprj0NHsAqVmO3Qku0COIpdyHU4lHctV0rHcat/XapFaNAs4KWiV9GAdf308ZNGbBQBA5fak5uitDUmSpOmjunF5PeoVwQmoIbufTe0jm6l9ZLNK2xiGoRyH83iQKg5TR7MdSs0uKF5WKmQdzS5QWm6hXIbcIwXWRHW9WS2CAxQR7K/mzQLUIjiAoAUAaDKeXL5dRS5DQ7q20rkdyv8RFDgVBCegHlksFoXY/RRi91O7lpUHrBJFTpeO5TqUmuXQ0ZwCd8gqCVan2pslSXY/q5qXhKngADVv5q+I4AA1L3ldZlnx8rBAf0YXBAD4lF/3Z+iTnw5IKr63CahvBCfARH42q6JCAxUVGlht28p6s9yvcxxKzSpQem6hjuU6lJ7rUKHTUEGRyz1Ee01ZLcWTE5cNXMWhqnTAijgeukraMfkwAMAsjy/bJkm69KxY9YgNN7kaNEYEJ8BH1LY3qyRopeU4lJ5bqLRch9Jyi58fyykOVmnHl6eX+je7oEguQ8fXFdaqxmYBtpPCVHHACrH7ye5nU6C/VYH+xf+WvLb722T3O77czyZ7SRu/4nWBflZGRAIAVOnbP1O1ZvsR+Vkt+seF9DahYRCcgEaqdNCKKz+IYKUcRa6TQlXZgFVR6ErPdchlSDkOp3Icedqfnlevn8XPalFgqYBlLxW83GHLr2wgK2lv97eVC2sl29hLbVPcttRzAhsA+ATDMPTYF8W9Tdf0O02ntQw2uSI0VgQnAGUE+FkVFRaoqLDqLx8s4XIZysovUlquw32ZYFrOiWCV4yhSfqFLBYVOFRS5lF/oVH6Rs3jZ8X/zS60rKHSVmay4yGUou6BI2QUN8YkrZ7NaToS142HK7g5dJ0LWiZBWNnjZ/St6XtH2pZcXh7kAm5XRoACgBpb9lqKfktMVHGDT3y7obHY5aMQITgBOmdVqUXiwv8KD/RWv6i8jrAmXyzgRpEqFrYLCkufFQSy/VJuCQmfZ9icFs9LbFBQ65ShyFW9Xst8ipwqdhrsGp8tQrsOpXIezXj5TbVgsKhPKAiu4pNFeSW/byb1ulfbWVbDen142AD6kyOly39t046D2ahVqN7kiNGYEJwBeyWq1KCjA5vHh1J0u43igKgljpZ6X6iUrXl6+XX5hZduWDWgVbZ9f5JRxPLcZho4HPlfVBdezinrZbFbL8YdVfsefu/+1nVhutRxfbrOc1O7EdmW2LVlnO/HaarGUeV3Re5788LNaZbOqbB22UvWU2t5a0X4sxf/Swwf4nvc27tOuIzlqHuyvm87vYHY5aOQITgBQis2kwCYVX6df6DSOX8boLBfGKvq3zCWOJ/W0FVTb7kSoK2FmL5vZSoeo0iGrqrDlZysbvKwWnfhXFlmtkvV4ILMeX261WGQp1a7sa4ssUqnlxesqfa0T2xW/94l2VosqDI0nAq5VNkv5z1bVZy4JyqU/f0UBtbKAW3J8gPqQX+jU3BU7JEmTh3ZSaKC/yRWhsSM4AYCXsFgsCvCzKMDPqjAP/gJgGEaZXq+yvWJOOV2GnC5DRSf9W/zcVX6d01Xmtavctq6T2p9Y7jRUvN558nu6yr13Rft3LzMMFTldchlyb+t0GXIZlR8Hp8uQU4bU9DKjR1ksKhX4igOmxVKy/ET4LNfu+L/SifBYur0kWa0n7aPk/cq91/GAWtLeejxoWoqfF/9bHEJLQqPFYjnes1kSVC3u57bjbUu2LQ7Scj+veP8nbVuqra3M5z9+3FS8oOSYnPhsx9eVblt6vU7s60Sbsse3on1V9F4ldZYEZlupY2Ur9blsJ30Wq1XHQ7rV3bbs9nUL069+u0cpmflqExGkv57brk77AGqD4AQATZzFYjk+8qBN4Wrcf7F1uQw5jVLhyzgR3FzGyUHMJaerOHi5XGUDmDvEldpeMmQYksuQXIYhQ8Wh1GUYcrkkQ8eXG5W0Ox7sSm9Xul1J/RXtv6SdcXx5yecs/XmLnKWel/qcJwdPd1AtFWJLjo2zgke5fRx/j8oYxz+jDON4Rq0izaLJsJ0Usqwn9e6WCVvHg2jJCK5/v7CLAv2ZRxANj+AEAGgyrFaLrLKI37EalmGUDVGlw5VRKuAZpUKg63jYKrPseCiUjodK48QyQyX7OjlI1q59yWvn8fDsdBW/LlnmPF6zs1Sb4kBaKqCWLHMVtyv9+UvC7on9Hw+/ZdaX2t/x5SWfu+SYuOs96XXJ+uLWJz6j+/nx3RjV7EuG4Y6wJx+v0seqzB8eynym0stO/MGhuiBdoq49vt1jwnRZrza12wioI4ITAACoV5bjl3PxSwZKlO79dIetkp7c472y7l7SCnpMywY1HQ+0hnq0CZetjpf6AbXFzzQAAAA0KHp70RgwYQcAAAAAVIPgBAAAAADVIDgBAAAAQDUITgAAAABQDYITAAAAAFSD4AQAAAAA1SA4AQAAAEA1CE4AAAAAUA2CEwAAAABUg+AEAAAAANUgOAEAAABANQhOAAAAAFANghMAAAAAVIPgBAAAAADV8DO7AE8zDEOSlJmZaXIlAAAAAMxUkglKMkJVmlxwysrKkiTFxcWZXAkAAAAAb5CVlaXw8PAq21iMmsSrRsTlcunAgQMKDQ2VxWIxuxxlZmYqLi5OycnJCgsLM7ucRo/j7Xkcc8/jmHsWx9vzOOaexzH3LI635xiGoaysLMXGxspqrfoupibX42S1WtW2bVuzyygnLCyM/zA8iOPteRxzz+OYexbH2/M45p7HMfcsjrdnVNfTVILBIQAAAACgGgQnAAAAAKgGwclkdrtds2bNkt1uN7uUJoHj7Xkcc8/jmHsWx9vzOOaexzH3LI63d2pyg0MAAAAAQG3R4wQAAAAA1SA4AQAAAEA1CE4AAAAAUA2CEwAAAABUg+DUwJ577jnFx8crMDBQ/fr104YNG6ps/+6776pbt24KDAxUz549tXTpUg9V6vtmz56tc845R6GhoYqKitKYMWO0bdu2KrdZtGiRLBZLmUdgYKCHKvZ9999/f7nj161btyq34Rw/NfHx8eWOucVi0eTJkytszzlee2vWrNHo0aMVGxsri8WiJUuWlFlvGIZmzpypmJgYBQUFafjw4dqxY0e1+63t/w+aiqqOd2FhoaZPn66ePXuqWbNmio2N1fjx43XgwIEq91mXn01NSXXn+MSJE8sdv1GjRlW7X87xylV3zCv6uW6xWDRnzpxK98l57nkEpwa0ePFiTZs2TbNmzdKmTZuUkJCgkSNH6vDhwxW2//bbb3X11Vfrhhtu0ObNmzVmzBiNGTNGv/76q4cr901ff/21Jk+erO+++07Lly9XYWGhRowYoZycnCq3CwsL08GDB92PvXv3eqjixqFHjx5ljt/atWsrbcs5fup++OGHMsd7+fLlkqS//OUvlW7DOV47OTk5SkhI0HPPPVfh+scff1zPPPOM5s+fr++//17NmjXTyJEjlZ+fX+k+a/v/g6akquOdm5urTZs26b777tOmTZv0wQcfaNu2bbrkkkuq3W9tfjY1NdWd45I0atSoMsfvrbfeqnKfnONVq+6Ylz7WBw8e1CuvvCKLxaKxY8dWuV/Ocw8z0GD69u1rTJ482f3a6XQasbGxxuzZsytsf+WVVxoXX3xxmWX9+vUzbrnllgats7E6fPiwIcn4+uuvK22zcOFCIzw83HNFNTKzZs0yEhISatyec7z+3XnnnUbHjh0Nl8tV4XrO8VMjyfjwww/dr10ul9G6dWtjzpw57mXp6emG3W433nrrrUr3U9v/HzRVJx/vimzYsMGQZOzdu7fSNrX92dSUVXTMJ0yYYFx66aW12g/neM3V5Dy/9NJLjQsuuKDKNpznnkePUwNxOBzauHGjhg8f7l5mtVo1fPhwrV+/vsJt1q9fX6a9JI0cObLS9qhaRkaGJKlFixZVtsvOzla7du0UFxenSy+9VL/99psnyms0duzYodjYWHXo0EHXXnutkpKSKm3LOV6/HA6HXn/9dV1//fWyWCyVtuMcrz+7d+9WSkpKmfM4PDxc/fr1q/Q8rsv/D1C5jIwMWSwWRUREVNmuNj+bUN7q1asVFRWlrl276rbbbtPRo0crbcs5Xr8OHTqkzz77TDfccEO1bTnPPYvg1EBSU1PldDoVHR1dZnl0dLRSUlIq3CYlJaVW7VE5l8ulqVOnauDAgTrjjDMqbde1a1e98sor+uijj/T666/L5XJpwIAB2rdvnwer9V39+vXTokWL9MUXX+iFF17Q7t27dd555ykrK6vC9pzj9WvJkiVKT0/XxIkTK23DOV6/Ss7V2pzHdfn/ASqWn5+v6dOn6+qrr1ZYWFil7Wr7swlljRo1Sq+99ppWrlypxx57TF9//bUSExPldDorbM85Xr9effVVhYaG6vLLL6+yHee55/mZXQDQECZPnqxff/212mt9+/fvr/79+7tfDxgwQN27d9eLL76ohx56qKHL9HmJiYnu52eeeab69eundu3a6Z133qnRX8pwal5++WUlJiYqNja20jac42gsCgsLdeWVV8owDL3wwgtVtuVn06m56qqr3M979uypM888Ux07dtTq1as1bNgwEytrGl555RVde+211Q7kw3nuefQ4NZDIyEjZbDYdOnSozPJDhw6pdevWFW7TunXrWrVHxaZMmaJPP/1Uq1atUtu2bWu1rb+/v3r16qWdO3c2UHWNW0REhLp06VLp8eMcrz979+7VihUrdOONN9ZqO87xU1NyrtbmPK7L/w9QVklo2rt3r5YvX15lb1NFqvvZhKp16NBBkZGRlR4/zvH6880332jbtm21/tkucZ57AsGpgQQEBKh3795auXKle5nL5dLKlSvL/PW3tP79+5dpL0nLly+vtD3KMgxDU6ZM0YcffqivvvpK7du3r/U+nE6nfvnlF8XExDRAhY1fdna2/vzzz0qPH+d4/Vm4cKGioqJ08cUX12o7zvFT0759e7Vu3brMeZyZmanvv/++0vO4Lv8/wAkloWnHjh1asWKFWrZsWet9VPezCVXbt2+fjh49Wunx4xyvPy+//LJ69+6thISEWm/Lee4BZo9O0Zi9/fbbht1uNxYtWmT8/vvvxs0332xEREQYKSkphmEYxnXXXWfcfffd7vbr1q0z/Pz8jCeeeMLYunWrMWvWLMPf39/45ZdfzPoIPuW2224zwsPDjdWrVxsHDx50P3Jzc91tTj7mDzzwgLFs2TLjzz//NDZu3GhcddVVRmBgoPHbb7+Z8RF8zj/+8Q9j9erVxu7du41169YZw4cPNyIjI43Dhw8bhsE53lCcTqdx2mmnGdOnTy+3jnP81GVlZRmbN282Nm/ebEgynnrqKWPz5s3uUdweffRRIyIiwvjoo4+Mn3/+2bj00kuN9u3bG3l5ee59XHDBBca8efPcr6v7/0FTVtXxdjgcxiWXXGK0bdvW2LJlS5mf7QUFBe59nHy8q/vZ1NRVdcyzsrKMu+66y1i/fr2xe/duY8WKFcbZZ59tdO7c2cjPz3fvg3O8dqr7uWIYhpGRkWEEBwcbL7zwQoX74Dw3H8Gpgc2bN8847bTTjICAAKNv377Gd9995143ePBgY8KECWXav/POO0aXLl2MgIAAo0ePHsZnn33m4Yp9l6QKHwsXLnS3OfmYT5061f39REdHGxdddJGxadMmzxfvo8aNG2fExMQYAQEBRps2bYxx48YZO3fudK/nHG8Yy5YtMyQZ27ZtK7eOc/zUrVq1qsKfJSXH1eVyGffdd58RHR1t2O12Y9iwYeW+i3bt2hmzZs0qs6yq/x80ZVUd7927d1f6s33VqlXufZx8vKv72dTUVXXMc3NzjREjRhitWrUy/P39jXbt2hk33XRTuQDEOV471f1cMQzDePHFF42goCAjPT29wn1wnpvPYhiG0aBdWgAAAADg47jHCQAAAACqQXACAAAAgGoQnAAAAACgGgQnAAAAAKgGwQkAAAAAqkFwAgAAAIBqEJwAAAAAoBoEJwAAAACoBsEJAIAqWCwWLVmyxOwyAAAmIzgBALzWxIkTZbFYyj1GjRpldmkAgCbGz+wCAACoyqhRo7Rw4cIyy+x2u0nVAACaKnqcAABezW63q3Xr1mUezZs3l1R8Gd0LL7ygxMREBQUFqUOHDnrvvffKbP/LL7/oggsuUFBQkFq2bKmbb75Z2dnZZdq88sor6tGjh+x2u2JiYjRlypQy61NTU3XZZZcpODhYnTt31scff+xel5aWpmuvvVatWrVSUFCQOnfuXC7oAQB8H8EJAODT7rvvPo0dO1Y//fSTrr32Wl111VXaunWrJCknJ0cjR45U8+bN9cMPP+jdd9/VihUrygSjF154QZMnT9bNN9+sX375RR9//LE6depU5j0eeOABXXnllfr555910UUX6dprr9WxY8fc7//777/r888/19atW/XCCy8oMjLScwcAAOARFsMwDLOLAACgIhMnTtTrr7+uwMDAMsvvuece3XPPPbJYLLr11lv1wgsvuNede+65Ovvss/X888/rpZde0vTp05WcnKxmzZpJkpYuXarRo0frwIEDio6OVps2bTRp0iT9+9//rrAGi8Wie++9Vw899JCk4jAWEhKizz//XKNGjdIll1yiyMhIvfLKKw10FAAA3oB7nAAAXm3o0KFlgpEktWjRwv28f//+Zdb1799fW7ZskSRt3bpVCQkJ7tAkSQMHDpTL5dK2bdtksVh04MABDRs2rMoazjzzTPfzZs2aKSwsTIcPH5Yk3XbbbRo7dqw2bdqkESNGaMyYMRowYECdPisAwHsRnAAAXq1Zs2blLp2rL0FBQTVq5+/vX+a1xWKRy+WSJCUmJmrv3r1aunSpli9frmHDhmny5Ml64okn6r1eAIB5uMcJAODTvvvuu3Kvu3fvLknq3r27fvrpJ+Xk5LjXr1u3TlarVV27dlVoaKji4+O1cuXKU6qhVatWmjBhgl5//XXNnTtXCxYsOKX9AQC8Dz1OAACvVlBQoJSUlDLL/Pz83AMwvPvuu+rTp48GDRqkN954Qxs2bNDLL78sSbr22ms1a9YsTZgwQffff7+OHDmiv/3tb7ruuusUHR0tSbr//vt16623KioqSomJicrKytK6dev0t7/9rUb1zZw5U71791aPHj1UUFCgTz/91B3cAACNB8EJAODVvvjiC8XExJRZ1rVrV/3xxx+Sike8e/vtt3X77bcrJiZGb731lk4//XRJUnBwsJYtW6Y777xT55xzjoKDgzV27Fg99dRT7n1NmDBB+fn5evrpp3XXXXcpMjJSV1xxRY3rCwgI0IwZM7Rnzx4FBQXpvPPO09tvv10PnxwA4E0YVQ8A4LMsFos+/PBDjRkzxuxSAACNHPc4AQAAAEA1CE4AAAAAUA3ucQIA+CyuNgcAeAo9TgAAAABQDYITAAAAAFSD4AQAAAAA1SA4AQAAAEA1CE4AAAAAUA2CEwAAAABUg+AEAAAAANUgOAEAAABANf4/rEiBZ0+8DV4AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluation Code\n",
        "checkpoint = torch.load('best_model.pth')\n",
        "model = MyTransformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward)\n",
        "model.load_state_dict(checkpoint)\n",
        "model.to(device)\n",
        "test_loss = evaluate(model, test_loader, criterion, device)\n",
        "print(f\"Test Loss: {test_loss}\")"
      ],
      "metadata": {
        "id": "UJYsRUt-i97z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d39f8ad-d184-4640-dab0-b77b7e697712"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 139/139 [00:38<00:00,  3.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0757004540118811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lZ1D0oGe-OkX"
      }
    }
  ]
}