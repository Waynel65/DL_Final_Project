# DL_Final_Project

**Group members: Jitendra Bhandari, Wei-Heng Lin, Akis Stravropoulos**

### Overview
This project is an attempt to recreate and improve the Foreign Accent Conversion model proposed by Waris Quamer, Et al in the Zero-Shot Foreign Accent Conversion without a Native Reference paper. More details can be found in our final report.

### Recreating our work:

*Please refer to https://github.com/warisqr007/ppg2ppg for detailed instructions on the installation of various dependencies and code to run the preprocessing and training parts, if one wishes to replicate the results.*

1. Dataset Downloading
- To recreate what we have done, one will need to download the recordings from the following sites:
- L1_speakers (American accent): http://festvox.org/cmu_arctic/cmu_arctic/packed/cmu_us_bdl_arctic-0.95-release.tar.bz2
- L2_speakers (Non-native accents):
    - First request the dataset at this site: https://psi.engr.tamu.edu/l2-arctic-corpus/
    - After that, an email will be sent and it will contain the links to download the dataset
    - To recreate our work, one will only need to download the package for speakers with these IDs: ABA, BWC, HJK, EBVS, HQTV
- Finally organize the folders in the following way:
    ```
        speakers/
        ├── L1_speakers/
            ├── BDL
        ├── L2_speakers/
            ├── ABA
            ├── BWC
            ├── HJK
            ├── EBVS
            ├── HQTV
    ```

2. BNF Generation
- BNFs (bottleneck features) are generated by the acoustic models, which are inputs into our translator and synthesizer models. 
- For the acoustic models, we used [Kaldi](https://github.com/kaldi-asr/kaldi) and the pretrained acoustic model provided by the paper's authors, which can be found [here](https://kaldi-asr.org/models/13/0013_librispeech_v1_chain.tar.gz). 
- One will need to install Kaldi using the link provided above before one can generate the BNFs 
- Before editing the script to make it work on one's local machine, there are some pretrained models will need to be downloaded:
    - all of the pretrained models can be downloaded [here](https://drive.google.com/file/d/1RUFXQ9jVXTAgPSukUuWv0TGKGhuaQeeo/view?usp=sharing)
    - after downloading, unzip the file to get pretrained_models directory and its structure should look something like this:
    ```
        pretrained_models/
        ├── synthesizer/
        ├── translator/
        └── pretrained_model/
    ```
- The script is provided in `wayne_kaldi_script` directory
- In the script, there are a few paths you will to have to change to make it work
- NOTE: when running the script, since it invokes a python script, be sure tha python 3.8 or above is used

3. Recreating the translator described in the paper
- We also trained our own version of a seq2seq based translator based on the code he provided
- One should be able to train their own translator after setting up the environment (i.e. installing the dependencies as described above)

a) Preprocessing:
```
    python synthesizer_preprocess_audio.py /path/to/L2-ARCTIC --out_dir=your_preprocess_output_dir
    python synthesizer_preprocess_embeds.py your_preprocess_output_dir
```
```
    python translator_preprocess_audio.py /path/to/L2-ARCTIC BDL /path/to/L2-ARCTIC/BDL/kaldi --out_dir=your_preprocess_output_dir
    python translator_preprocess_embeds.py your_preprocess_output_dir
```

b) Training:
```
    python translator_train.py PPG2PPG_train your_preprocess_output_dir (Successfully Done, but due to restriction on GPUs have to limit the epochs)
    python synthesizer_train.py Accetron_train your_preprocess_output_dir (We were not able to run this)
```

4. Recreating our version of translator
- We had to use google colab for this part, but if one's machine is powerful enough a local machine can also be used
- open `inference_script_wayne.ipynb` on google colab or local machine, and the instruction inside should help you navigate through our work


5. Perceptual experiment
- Although we did not manage to finish a transformed-based synthesizer, we still made a jupyter notebook that will supposedly allow us to conduct survey on how good our model is compared to the Warris Quamer's model
- The Perceptual experiment contains a jupyter notebook that can go over a list of recordings from the L1, L2 speakers and their accent-converted speech. The user will then be able to rate whether they think the accent sounds native and whether their original speaker identity is preserved